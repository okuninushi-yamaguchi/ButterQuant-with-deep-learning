# -*- coding: utf-8 -*-
"""
ButterQuant ML Training Data Generator / ButterQuant ML è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨
å†å²å›æµ‹æ¨¡æ‹Ÿç”Ÿæˆè®­ç»ƒæ•°æ® - Phase 1 / Historical simulation for training data - Phase 1

åŠŸèƒ½ / Features:
1. VIXåˆ†å±‚é‡‡æ · (ä½æ³¢/å¸¸æ€/é«˜æ³¢) / VIX-based stratified sampling (low/normal/high volatility)
2. å†å²è´è¶ç­–ç•¥æ¨¡æ‹Ÿ (as-ofåˆ†æ) / Historical butterfly strategy simulation (as-of analysis)
3. ç®€åŒ–IV Proxyè®¡ç®— / Simplified IV Proxy calculation
4. 14å¤©å‰å‘æ ‡æ³¨ (4åˆ†ç±») / 14-day forward labeling (4-class)

é¢„è®¡è€—æ—¶ / Estimated time: 2-3å°æ—¶ / 2-3 hours
è¾“å‡º / Output: ml/training_data_deep.parquet
"""

import os
import sys
import pandas as pd
import numpy as np
import yfinance as yf
from datetime import datetime, timedelta
import logging
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„ / Add project path
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))
sys.path.insert(0, str(PROJECT_ROOT / 'backend'))
import random
from typing import Dict, List, Tuple, Optional
from ml.features import calculate_dynamic_evaluation_date, classify_roi
import warnings
warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿— / Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logging.getLogger('yfinance').setLevel(logging.CRITICAL)  # Suppress yfinance warnings
logging.getLogger('peewee').setLevel(logging.CRITICAL)
logger = logging.getLogger(__name__)


class HistoricalDataGenerator:
    """å†å²å›æµ‹æ•°æ®ç”Ÿæˆå™¨ / Historical simulation data generator"""
    
    def __init__(self, output_dir: str = None):
        # è¾“å‡ºç›®å½• / Output directory
        if output_dir is None:
            self.output_dir = Path(__file__).parent
        else:
            self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # é‡‡æ ·é…ç½® / Sampling configuration
        self.sample_config = {
            'LOW_VOL': 2000,    # Target sample count
            'NORMAL': 3000,
            'HIGH_VOL': 2000
        }
        
        # VIXé˜ˆå€¼å®šä¹‰ / VIX threshold definitions
        self.vix_thresholds = {
            'LOW_VOL': (0, 15),
            'NORMAL': (15, 25),
            'HIGH_VOL': (25, 100)
        }
        
        # æ—¶é—´èŒƒå›´ / Time range
        self.start_date = "2023-01-01"
        self.end_date = "2025-01-31"
        
        # é«˜æµåŠ¨æ€§è‚¡ç¥¨æ±  (æŒ‰è¡Œä¸šåˆ†æ•£) / High liquidity stock pool (diversified by sector)
        self.ticker_pool = [
            # é«˜æµåŠ¨æ€§ETF (é‡è¦: æä¾›å¸‚åœºBetaç‰¹å¾) / High liquidity ETFs (Important: Provide market Beta)
            'SPY', 'QQQ', 'IWM', 'DIA', 'TLT', 'GLD', 'SLV', 'EEM', 'XLE', 'XLF', 'XLK', 'XLV',
            # ç§‘æŠ€ & åŠå¯¼ä½“ / Tech & Semi
            'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META', 'NVDA', 'TSLA', 'AMD', 'INTC', 'CRM',
            'ADBE', 'ORCL', 'CSCO', 'AVGO', 'QCOM', 'TXN', 'IBM', 'NOW', 'UBER', 'ABNB',
            'PLTR', 'SNOW', 'PANW', 'FTNT',
            # é‡‘è & æ”¯ä»˜ / Finance & Payments
            'JPM', 'BAC', 'WFC', 'GS', 'MS', 'C', 'BLK', 'SCHW', 'AXP', 'V', 'MA',
            'PYPL', 'SQ', 'COIN', 'HOOD',
            # åŒ»ç–— & åˆ¶è¯ / Healthcare & Pharma
            'JNJ', 'UNH', 'PFE', 'ABBV', 'TMO', 'MRK', 'LLY', 'BMY', 'AMGN', 'GILD',
            'CVS', 'CI', 'ISRG',
            # æ¶ˆè´¹ & é›¶å”® / Consumer & Retail
            'WMT', 'HD', 'NKE', 'MCD', 'COST', 'TGT', 'SBUX', 'LOW', 'PG', 'KO', 'PEP',
            'CL', 'EL', 'LULU', 'CMG',
            # èƒ½æº & åŸææ–™ / Energy & Materials
            'XOM', 'CVX', 'COP', 'SLB', 'EOG', 'OXY', 'LIN', 'FCX', 'NEM',
            # å·¥ä¸š & å›½é˜² / Industrial & Defense
            'BA', 'CAT', 'GE', 'UPS', 'HON', 'RTX', 'LMT', 'DE', 'UNP', 'LUV', 'DAL',
            # é€šè®¯ & åª’ä½“ / Telecom & Media
            'DIS', 'NFLX', 'CMCSA', 'TMUS', 'VZ', 'T',
            # æˆ¿åœ°äº§ & å…¬ç”¨äº‹ä¸š / Real Estate & Utilities
            'PLD', 'AMT', 'CCI', 'O', 'NEE', 'DUK', 'SO'
        ]
        
    def download_vix_data(self) -> pd.DataFrame:
        """ä¸‹è½½VIXå†å²æ•°æ® / Download VIX historical data"""
        logger.info("ğŸ“¥ ä¸‹è½½VIXå†å²æ•°æ®... / Downloading VIX historical data...")
        vix = yf.download("^VIX", start=self.start_date, end=self.end_date, progress=False)
        logger.info(f"âœ… è·å– {len(vix)} å¤©VIXæ•°æ® / Retrieved {len(vix)} days of VIX data")
        return vix
    
    def stratified_sampling(self, vix_data: pd.DataFrame) -> Dict[str, List[datetime]]:
        """åŸºäºVIXçš„åˆ†å±‚é‡‡æ · / VIX-based stratified sampling"""
        logger.info("ğŸ² æ‰§è¡Œåˆ†å±‚é‡‡æ ·... / Executing stratified sampling...")
        
        samples = {}
        for regime, (low, high) in self.vix_thresholds.items():
            # ç­›é€‰ç¬¦åˆæ¡ä»¶çš„æ—¥æœŸ / Filter eligible dates
            mask = (vix_data['Close'] >= low) & (vix_data['Close'] < high)
            eligible_dates = vix_data[mask].index.tolist()
            
            # è®¡ç®—éœ€è¦é‡‡æ ·çš„å¤©æ•° (æ¯å¤©50ä¸ªæ ‡çš„) / Calculate days to sample (50 tickers per day)
            n_days = self.sample_config[regime] // 50
            
            if len(eligible_dates) < n_days:
                logger.warning(f"âš ï¸ {regime} å¯ç”¨æ—¥æœŸä¸è¶³ / Insufficient eligible dates: {len(eligible_dates)} < {n_days}")
                sampled = eligible_dates
            else:
                sampled = random.sample(eligible_dates, k=n_days)
            
            samples[regime] = sorted(sampled)
            logger.info(f"  {regime}: é‡‡æ · {len(sampled)} å¤© (VIX {low}-{high}) / Sampled {len(sampled)} days")
        
        return samples
    
    def get_top_tickers(self, date: datetime, n: int = 50) -> List[str]:
        """
        è·å–æŒ‡å®šæ—¥æœŸçš„Top Næ ‡çš„ / Get Top N tickers for specified date
        
        ç®€åŒ–ç‰ˆ: ä½¿ç”¨å›ºå®šçš„æµåŠ¨æ€§å¥½çš„è‚¡ç¥¨æ±  / Simplified: Use fixed high-liquidity stock pool
        ç”Ÿäº§ç‰ˆ: å¯ä»¥ä»å†å²å¸‚å€¼/æˆäº¤é‡æ•°æ®ç­›é€‰ / Production: Filter by historical market cap/volume
        """
        # éšæœºæŠ½å–nä¸ª (æ¨¡æ‹Ÿä¸åŒæ—¥æœŸçš„çƒ­é—¨è‚¡) / Random sample n (simulate popular stocks on different dates)
        return random.sample(self.ticker_pool, min(n, len(self.ticker_pool)))
    
    def get_iv_proxy(self, strike: float, spot: float, hv: float) -> float:
        """
        ç®€åŒ–ç‰ˆIVä»£ç† / Simplified IV Proxy
        
        åŸºäºMoneynessè°ƒæ•´å†å²æ³¢åŠ¨ç‡ / Adjust historical volatility based on Moneyness:
        - OTM Put (K < 0.95S): 1.25x HV (ææ…Œæº¢ä»· / Panic premium)
        - ATM (0.95S â‰¤ K â‰¤ 1.05S): 1.15x HV
        - OTM Call (K > 1.05S): 1.10x HV
        """
        moneyness = strike / spot
        
        if moneyness < 0.95:
            multiplier = 1.25
        elif moneyness > 1.05:
            multiplier = 1.10
        else:
            multiplier = 1.15
        
        return hv * multiplier
    
    def simulate_butterfly_analysis(self, ticker: str, as_of_date: datetime) -> Optional[Dict]:
        """
        æ¨¡æ‹Ÿè´è¶ç­–ç•¥åˆ†æ (as-ofæ—¶åˆ») / Simulate butterfly strategy analysis (as-of moment)
        
        æ³¨æ„: è¿™é‡Œç®€åŒ–äº†åˆ†æé€»è¾‘,å®é™…å¯é›†æˆButterflyAnalyzer / Note: Simplified analysis logic
        """
        try:
            # 1. è·å–æˆªè‡³è¯¥æ—¥æœŸçš„å†å²ä»·æ ¼ / Get historical prices up to that date
            hist = yf.download(
                ticker, 
                end=as_of_date, 
                period="90d",
                progress=False,
                threads=False,  # Reduce rate limit/errors
                ignore_tz=True  # Fix timezone issues
            )
            
            if len(hist) < 30:
                return None
            
            spot = float(hist['Close'].iloc[-1])
            returns = hist['Close'].pct_change().dropna()
            hv = float(returns.std() * np.sqrt(252))
            
            # 2. æ„é€ è´è¶ç­–ç•¥å‚æ•° / Construct butterfly strategy parameters
            dte = 30  # å‡è®¾30å¤©åˆ°æœŸ / Assume 30 days to expiry
            
            # ç¡®å®šè¡Œæƒä»·é—´éš” / Determine strike interval
            if spot < 50:
                strike_step = 2.5
            elif spot < 100:
                strike_step = 5
            elif spot < 200:
                strike_step = 5
            else:
                strike_step = 10
            
            center_strike = round(spot / strike_step) * strike_step
            wing_width = strike_step * 2  # 2ä¸ªé—´éš”çš„ç¿¼å®½ / 2 intervals wing width
            
            lower_strike = center_strike - wing_width
            upper_strike = center_strike + wing_width
            
            # 3. ä½¿ç”¨IV Proxyè®¡ç®—æœŸæƒä»·æ ¼ / Calculate option prices using IV Proxy
            iv_lower = self.get_iv_proxy(lower_strike, spot, hv)
            iv_center = self.get_iv_proxy(center_strike, spot, hv)
            iv_upper = self.get_iv_proxy(upper_strike, spot, hv)
            
            # ç®€åŒ–BSå®šä»· / Simplified BS pricing
            T = dte / 365
            r = 0.045  # æ— é£é™©åˆ©ç‡ / Risk-free rate
            
            def simple_call_price(S, K, iv, T):
                """ç®€åŒ–çš„CallæœŸæƒä»·æ ¼ / Simplified Call option price"""
                from scipy.stats import norm
                if T <= 0 or iv <= 0:
                    return max(S - K, 0)
                d1 = (np.log(S/K) + (r + 0.5*iv**2)*T) / (iv*np.sqrt(T))
                d2 = d1 - iv*np.sqrt(T)
                return S * norm.cdf(d1) - K * np.exp(-r*T) * norm.cdf(d2)
            
            price_lower = simple_call_price(spot, lower_strike, iv_lower, T)
            price_center = simple_call_price(spot, center_strike, iv_center, T)
            price_upper = simple_call_price(spot, upper_strike, iv_upper, T)
            
            # è´è¶ç»„åˆæˆæœ¬ / Butterfly spread cost
            net_debit = price_lower - 2*price_center + price_upper
            net_debit = max(0.10, net_debit)  # æœ€ä½æˆæœ¬ / Minimum cost
            max_profit = wing_width - net_debit
            
            # 4. è®¡ç®—ç‰¹å¾ / Calculate features
            returns_arr = returns.values
            forecast_price = spot * (1 + float(returns.mean()) * dte)
            predicted_vol = hv * 0.9  # GARCHé¢„æµ‹é€šå¸¸ç•¥ä½ / GARCH prediction usually slightly lower
            
            # å‚…é‡Œå¶ç›¸å…³ç‰¹å¾ (ç®€åŒ–) / Fourier-related features (simplified)
            if len(returns_arr) >= 20:
                trend_slope = float((hist['Close'].iloc[-1] - hist['Close'].iloc[-20]) / hist['Close'].iloc[-20] * 100)
            else:
                trend_slope = 0.0
            
            # Greeks (ç®€åŒ–è®¡ç®—) / Greeks (simplified calculation)
            delta = np.random.normal(0.0, 0.01)  # åŠ ä¸Šå¾®å°æ‰°åŠ¨ / Add small noise
            gamma = 0.05 / spot
            vega = wing_width * 0.01
            theta = -net_debit / dte
            
            # åŠ¨é‡å’Œæˆäº¤é‡ç‰¹å¾ / Momentum and volume features
            if len(hist) >= 7:
                momentum_7d = float((hist['Close'].iloc[-1] - hist['Close'].iloc[-7]) / hist['Close'].iloc[-7])
                vol_recent = hist['Volume'].iloc[-5:]
                vol_concentration = float(vol_recent.max() / (vol_recent.mean() + 1e-6))
            else:
                momentum_7d = 0.0
                vol_concentration = 1.0
            
            return {
                'ticker': ticker,
                'analysis_date': as_of_date,
                'spot_price': spot,
                'hv': hv,
                
                'butterfly': {
                    'lower_strike': lower_strike,
                    'center_strike': center_strike,
                    'upper_strike': upper_strike,
                    'net_debit': net_debit,
                    'max_profit': max_profit,
                    'max_loss': net_debit,
                    'profit_ratio': max_profit / net_debit if net_debit > 0 else 0,
                    'prob_profit': 0.5,  # ç®€åŒ– / Simplified
                    'dte': dte
                },
                
                'fourier': {
                    'trend_slope': trend_slope,
                    'dominant_period_days': 21,
                    'period_strength': 0.3
                },
                
                'arima': {
                    'mean_forecast': forecast_price,
                    'confidence_interval_width': spot * 0.1
                },
                
                'garch': {
                    'predicted_vol': predicted_vol,
                    'current_iv': iv_center,
                    'vol_mispricing': (iv_center - predicted_vol) / predicted_vol if predicted_vol > 0 else 0,
                    'iv_percentile': 50.0 + np.random.normal(0, 5)  # æ¨¡æ‹ŸIVåˆ†ä½æ•°çš„éšæœºæ€§ / Simulate IV percentile randomness
                },
                
                'greeks': {
                    'delta': delta,
                    'gamma': gamma,
                    'vega': vega,
                    'theta': theta
                },
                
                # é¢å¤–ç‰¹å¾ / Extra features
                'momentum_7d': momentum_7d,
                'vol_concentration': vol_concentration
            }
            
        except Exception as e:
            logger.debug(f"âš ï¸ {ticker} @ {as_of_date}: {e}")
            return None
    
    def calculate_label(self, analysis: Dict, future_price: float) -> int:
        """
        è®¡ç®—4åˆ†ç±»æ ‡ç­¾ / Calculate 4-class label
        
        åŸºäº14å¤©åçš„å®é™…ä»·æ ¼è®¡ç®—ROI / Calculate ROI based on actual price after 14 days:
        - 0: äºæŸ (ROI < 0) / Loss
        - 1: å¾®åˆ© (0% â‰¤ ROI < 10%) / Minor profit
        - 2: è‰¯å¥½ (10% â‰¤ ROI < 30%) / Good
        - 3: ä¼˜ç§€ (ROI â‰¥ 30%) / Excellent
        """
        bf = analysis['butterfly']
        lower = bf['lower_strike']
        center = bf['center_strike']
        upper = bf['upper_strike']
        cost = bf['net_debit']
        max_profit = bf['max_profit']
        
        # è®¡ç®—payoff / Calculate payoff
        if lower <= future_price <= upper:
            if future_price <= center:
                # å·¦ç¿¼ / Left wing
                payoff = max_profit * (future_price - lower) / (center - lower) if center != lower else 0
            else:
                # å³ç¿¼ / Right wing
                payoff = max_profit * (upper - future_price) / (upper - center) if upper != center else 0
        else:
            # è¶…å‡ºåŒºé—´,æŸå¤±å…¨éƒ¨æˆæœ¬ / Outside range, lose all cost
            payoff = -cost
        
        roi = (payoff - cost) / cost if cost > 0 else -1
        
        # ä½¿ç”¨ç»Ÿä¸€çš„æ–°åˆ†ç±»æ ‡å‡† / Use unified new classification criteria
        label = classify_roi(roi)
        return label, roi
    
    def extract_features(self, analysis: Dict) -> Dict:
        """
        æå–23ç»´ç‰¹å¾å‘é‡ / Extract 23-dim feature vector
        
        åŒ…æ‹¬ / Includes:
        - åŸæœ‰16ä¸ªç‰¹å¾ (ç§»é™¤total_score) / Original 16 features (removed total_score)
        - æ–°å¢6ä¸ªä½æˆæœ¬ç‰¹å¾ / 6 new low-cost features
        """
        bf = analysis['butterfly']
        fourier = analysis['fourier']
        arima = analysis['arima']
        garch = analysis['garch']
        greeks = analysis['greeks']
        
        return {
            # åŸæœ‰ç‰¹å¾ (16ä¸ª) / Original features (16)
            'trend_slope': fourier.get('trend_slope', 0),
            'dominant_period': fourier.get('dominant_period_days', 0),
            'period_strength': fourier.get('period_strength', 0),
            'forecast_price': arima.get('mean_forecast', 0),
            'predicted_vol': garch.get('predicted_vol', 0),
            'current_iv': garch.get('current_iv', 0),
            'vol_mispricing': garch.get('vol_mispricing', 0),
            'iv_percentile': garch.get('iv_percentile', 0),
            'delta': greeks.get('delta', 0),
            'gamma': greeks.get('gamma', 0),
            'vega': greeks.get('vega', 0),
            'theta': greeks.get('theta', 0),
            'max_profit': bf.get('max_profit', 0),
            'max_loss': bf.get('max_loss', 0),
            'profit_ratio': bf.get('profit_ratio', 0),
            'prob_profit': bf.get('prob_profit', 0.5),
            
            # æ–°å¢ç‰¹å¾ (6ä¸ª) / New features (6)
            'skew_estimate': garch.get('vol_mispricing', 0) * 100,
            'momentum_7d': analysis.get('momentum_7d', 0),
            'vol_concentration': analysis.get('vol_concentration', 1.0),
            'dte_factor': bf.get('dte', 30) / 30.0,
            'price_stability': 1.0 / (arima.get('confidence_interval_width', 1.0) + 1e-6),
            'gamma_theta_ratio': abs(greeks.get('gamma', 0) / (greeks.get('theta', -0.01) + 1e-6))
        }
    
    def generate_dataset(self, limit: int = None) -> pd.DataFrame:
        """ä¸»æ•°æ®ç”Ÿæˆæµç¨‹ / Main data generation process"""
        
        # Step 1: VIXåˆ†å±‚é‡‡æ · / VIX stratified sampling
        vix_data = self.download_vix_data()
        sampled_dates = self.stratified_sampling(vix_data)
        
        # Step 2: éå†æ‰€æœ‰é‡‡æ ·æ—¥æœŸ / Iterate through all sampled dates
        all_samples = []
        total_dates = sum(len(dates) for dates in sampled_dates.values())
        
        logger.info(f"ğŸš€ å¼€å§‹ç”Ÿæˆæ•°æ®: å…± {total_dates} å¤© / Starting data generation: {total_dates} days total")
        
        processed = 0
        total_collected = 0
        
        for regime, dates in sampled_dates.items():
            if limit and total_collected >= limit:
                break
                
            logger.info(f"\nğŸ“Š å¤„ç† {regime} å¸‚åœº ({len(dates)} å¤©) / Processing {regime} market ({len(dates)} days)")
            
            for date in dates:
                if limit and total_collected >= limit:
                    break
                    
                processed += 1
                if processed % 5 == 0:
                    logger.info(f"  [{processed}/{total_dates}] {date.date() if hasattr(date, 'date') else date}")
                
                # è·å–Top 50æ ‡çš„ / Get Top 50 tickers
                tickers = self.get_top_tickers(date)
                
                for ticker in tickers:
                    if limit and total_collected >= limit:
                        break
                        
                    try:
                        # å†å²æ¨¡æ‹Ÿåˆ†æ / Historical simulation analysis
                        analysis = self.simulate_butterfly_analysis(ticker, date)
                        if analysis is None:
                            continue
                        
                        # è·å–è´è¶å‚æ•° / Get butterfly params
                        bf = analysis['butterfly']
                        dte = bf['dte']
                        
                        # åŠ¨æ€è®¡ç®—è¯„ä¼°æ—¥æœŸ / Calculate dynamic evaluation date
                        future_date, _ = calculate_dynamic_evaluation_date(date, dte)
                        
                        # è·å–è¯„ä¼°æ—¥æœŸçš„å®é™…ä»·æ ¼ / Get actual price at evaluation date
                        future_data = yf.download(
                            ticker,
                            start=future_date,
                            end=future_date + timedelta(days=5),
                            progress=False,
                            threads=False,
                            ignore_tz=True
                        )
                        
                        if len(future_data) == 0:
                            continue
                        
                        # Fix for multi-level columns
                        close_data = future_data['Close']
                        if isinstance(close_data, pd.DataFrame):
                            future_price = float(close_data.iloc[0, 0])
                        else:
                            future_price = float(close_data.iloc[0])
                        
                        # è®¡ç®—æ ‡ç­¾ / Calculate label (using new classify_roi)
                        label, roi_val = self.calculate_label(analysis, future_price)
                        
                        # æå–ç‰¹å¾ / Extract features
                        features = self.extract_features(analysis)
                        
                        # åˆå¹¶ / Merge
                        sample = {
                            **features,
                            'label': label,
                            '_ticker': ticker,
                            '_date': str(date.date() if hasattr(date, 'date') else date),
                            '_regime': regime,
                            '_spot': analysis['spot_price'],
                            '_future_price': future_price,
                            '_debug_roi': roi_val,
                            # Save strikes for future safety
                            'lower_strike': bf['lower_strike'],
                            'center_strike': bf['center_strike'],
                            'upper_strike': bf['upper_strike'],
                            'net_debit': bf['net_debit'],
                            'dte': dte
                        }
                        
                        all_samples.append(sample)
                        total_collected += 1
                        
                    except Exception as e:
                        logger.debug(f"    âš ï¸ {ticker}: {e}")
                        continue
                
                if processed % 10 == 0:
                    logger.info(f"  âœ… å·²æ”¶é›† {len(all_samples)} ä¸ªæ ·æœ¬ / Collected {len(all_samples)} samples")
        
        # Step 3: è½¬æ¢ä¸ºDataFrame / Convert to DataFrame
        df = pd.DataFrame(all_samples)
        logger.info(f"\nâœ… æ•°æ®ç”Ÿæˆå®Œæˆ: {len(df)} ä¸ªæ ·æœ¬ / Data generation complete: {len(df)} samples")
        
        return df
    
    def validate_dataset(self, df: pd.DataFrame):
        """æ•°æ®è´¨é‡éªŒè¯ / Data quality validation"""
        logger.info("\nğŸ” æ•°æ®è´¨é‡æ£€æŸ¥ / Data Quality Check:")
        
        # 1. æ ·æœ¬é‡ / Sample count
        logger.info(f"  æ€»æ ·æœ¬æ•° / Total samples: {len(df)}")
        
        # 2. æ ‡ç­¾åˆ†å¸ƒ / Label distribution
        label_dist = df['label'].value_counts(normalize=True).sort_index()
        logger.info(f"  æ ‡ç­¾åˆ†å¸ƒ / Label distribution:")
        label_names = ['Loss/äºæŸ', 'Minor/å¾®åˆ©', 'Good/è‰¯å¥½', 'Excellent/ä¼˜ç§€']
        for label, pct in label_dist.items():
            logger.info(f"    Class {label} ({label_names[label]}): {pct:.1%}")
        
        # 3. ç¼ºå¤±å€¼ / Missing values
        missing = df.isnull().sum().sum()
        logger.info(f"  ç¼ºå¤±å€¼ / Missing values: {missing}")
        
        # 4. ç‰¹å¾èŒƒå›´ / Feature range
        feature_cols = [col for col in df.columns if not col.startswith('_') and col != 'label']
        logger.info(f"  ç‰¹å¾æ•°é‡ / Feature count: {len(feature_cols)}")
        
        # æ£€æŸ¥å¼‚å¸¸å€¼ / Check for anomalies
        for col in feature_cols:
            if np.isinf(df[col]).any():
                logger.warning(f"  âš ï¸ {col} åŒ…å«æ— ç©·å€¼ / contains infinite values")
            if df[col].std() == 0:
                logger.warning(f"  âš ï¸ {col} æ— æ–¹å·® / has no variance")
    
    def run(self, limit: int = None) -> pd.DataFrame:
        """æ‰§è¡Œå®Œæ•´æµç¨‹ / Execute complete pipeline"""
        logger.info("=" * 60)
        logger.info("ğŸ¦‹ ButterQuant ML è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨ / Training Data Generator")
        logger.info("=" * 60)
        
        # ç”Ÿæˆæ•°æ® / Generate data
        df = self.generate_dataset(limit=limit)
        
        if len(df) == 0:
            logger.error("âŒ æœªç”Ÿæˆä»»ä½•æ•°æ®! / No data generated!")
            return df
        
        # éªŒè¯ / Validate
        self.validate_dataset(df)
        
        # ä¿å­˜ / Save
        try:
            output_path = self.output_dir / "training_data_deep.parquet"
            if limit:
                output_path = self.output_dir / "training_data_deep_test.parquet"
            
            df.to_parquet(output_path, index=False)
            logger.info(f"\nğŸ’¾ æ•°æ®å·²ä¿å­˜(Parquet) / Data saved: {output_path}")
        except ImportError:
            logger.warning("âš ï¸ ç¼ºå°‘pyarrow/fastparquetï¼Œå›é€€åˆ°CSV / Missing parquet lib, fallback to CSV")
            output_path = self.output_dir / "training_data_deep.csv"
            df.to_csv(output_path, index=False)
            logger.info(f"\nğŸ’¾ æ•°æ®å·²ä¿å­˜(CSV) / Data saved: {output_path}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜å¤±è´¥ / Save failed: {e}")
            # å°è¯•å¼ºåˆ¶ä¿å­˜CSV / Try force CSV
            output_path = self.output_dir / "training_data_deep_backup.csv"
            df.to_csv(output_path, index=False)
            logger.info(f"ğŸ’¾ å·²å¼ºåˆ¶ä¿å­˜å¤‡ä»½ / Backup saved: {output_path}")
        
        # åŒæ—¶ä¿å­˜CSVé¢„è§ˆ / Also save CSV preview
        csv_path = self.output_dir / "training_data_deep.csv"
        df.head(100).to_csv(csv_path, index=False)
        logger.info(f"ğŸ’¾ æ ·æœ¬é¢„è§ˆå·²ä¿å­˜ / Sample preview saved: {csv_path}")
        
        # æ ·æœ¬é¢„è§ˆ / Sample preview
        logger.info("\nğŸ“‹ æ ·æœ¬é¢„è§ˆ / Sample Preview:")
        print(df.head())
        
        return df


def main():
    import argparse
    parser = argparse.ArgumentParser(description="ButterQuant ML Data Generator")
    parser.add_argument('--limit', type=int, default=None, help='Limit total samples for testing')
    args = parser.parse_args()
    
    generator = HistoricalDataGenerator()
    generator.run(limit=args.limit)


if __name__ == "__main__":
    main()
