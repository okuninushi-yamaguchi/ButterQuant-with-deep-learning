# -*- coding: utf-8 -*-
"""
ButterQuant æ•°æ®è´¨é‡éªŒè¯è„šæœ¬ / Data Quality Validation Script
éªŒè¯è®­ç»ƒæ•°æ®çš„å®Œæ•´æ€§å’Œè´¨é‡ / Validate training data integrity and quality

ç”¨æ³• / Usage:
    python ml/validate_data.py
    python ml/validate_data.py --file training_data_deep.parquet
"""

import pandas as pd
import numpy as np
from pathlib import Path
import logging
import argparse
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„ / Add project path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from ml.features import FeatureExtractor, validate_feature_quality

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class DataValidator:
    """è®­ç»ƒæ•°æ®éªŒè¯å™¨ / Training Data Validator"""
    
    def __init__(self, data_path: str):
        self.data_path = Path(data_path)
        self.df = None
        self.issues = []
        self.warnings = []
        
    def load_data(self) -> bool:
        """åŠ è½½æ•°æ® / Load data"""
        logger.info(f"ğŸ“¥ åŠ è½½æ•°æ®: {self.data_path}")
        
        if not self.data_path.exists():
            logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {self.data_path}")
            return False
        
        try:
            if self.data_path.suffix == '.parquet':
                self.df = pd.read_parquet(self.data_path)
            elif self.data_path.suffix == '.csv':
                self.df = pd.read_csv(self.data_path)
            else:
                logger.error(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {self.data_path.suffix}")
                return False
            
            logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(self.df)} è¡Œæ•°æ®")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½å¤±è´¥: {e}")
            return False
    
    def validate_shape(self):
        """éªŒè¯æ•°æ®å½¢çŠ¶ / Validate data shape"""
        logger.info("\nğŸ“ æ£€æŸ¥æ•°æ®å½¢çŠ¶...")
        
        n_rows, n_cols = self.df.shape
        logger.info(f"  è¡Œæ•°: {n_rows}")
        logger.info(f"  åˆ—æ•°: {n_cols}")
        
        # æœ€å°æ ·æœ¬æ•°æ£€æŸ¥ / Minimum sample check
        if n_rows < 1000:
            self.warnings.append(f"æ ·æœ¬æ•°è¾ƒå°‘: {n_rows} (å»ºè®® > 5000)")
        elif n_rows < 5000:
            self.warnings.append(f"æ ·æœ¬æ•°åå°‘: {n_rows} (å»ºè®® > 5000)")
        else:
            logger.info(f"  âœ… æ ·æœ¬æ•°å……è¶³")
    
    def validate_features(self):
        """éªŒè¯ç‰¹å¾å®Œæ•´æ€§ / Validate feature completeness"""
        logger.info("\nğŸ” æ£€æŸ¥ç‰¹å¾å®Œæ•´æ€§...")
        
        expected_features = FeatureExtractor.FEATURE_NAMES
        
        # æ£€æŸ¥ç¼ºå¤±ç‰¹å¾ / Check missing features
        missing = []
        for feat in expected_features:
            if feat not in self.df.columns:
                missing.append(feat)
        
        if missing:
            self.issues.append(f"ç¼ºå¤±ç‰¹å¾: {missing}")
            logger.error(f"  âŒ ç¼ºå¤± {len(missing)} ä¸ªç‰¹å¾: {missing}")
        else:
            logger.info(f"  âœ… æ‰€æœ‰ {len(expected_features)} ä¸ªç‰¹å¾éƒ½å­˜åœ¨")
        
        # æ£€æŸ¥æ ‡ç­¾åˆ— / Check label column
        if 'label' not in self.df.columns:
            self.issues.append("ç¼ºå¤±æ ‡ç­¾åˆ— 'label'")
            logger.error(f"  âŒ ç¼ºå¤±æ ‡ç­¾åˆ— 'label'")
        else:
            logger.info(f"  âœ… æ ‡ç­¾åˆ—å­˜åœ¨")
    
    def validate_values(self):
        """éªŒè¯æ•°æ®å€¼ / Validate data values"""
        logger.info("\nğŸ“Š æ£€æŸ¥æ•°æ®å€¼...")
        
        feature_cols = [c for c in FeatureExtractor.FEATURE_NAMES if c in self.df.columns]
        
        # NaNæ£€æŸ¥ / NaN check
        nan_counts = self.df[feature_cols].isna().sum()
        total_nan = nan_counts.sum()
        
        if total_nan > 0:
            nan_features = nan_counts[nan_counts > 0]
            self.warnings.append(f"å‘ç° {total_nan} ä¸ªNaNå€¼")
            logger.warning(f"  âš ï¸ å‘ç° {total_nan} ä¸ªNaNå€¼:")
            for feat, count in nan_features.items():
                logger.warning(f"    - {feat}: {count}")
        else:
            logger.info(f"  âœ… æ— NaNå€¼")
        
        # Infæ£€æŸ¥ / Inf check
        inf_count = 0
        for col in feature_cols:
            inf_count += np.isinf(self.df[col]).sum()
        
        if inf_count > 0:
            self.issues.append(f"å‘ç° {inf_count} ä¸ªInfå€¼")
            logger.error(f"  âŒ å‘ç° {inf_count} ä¸ªInfå€¼")
        else:
            logger.info(f"  âœ… æ— Infå€¼")
        
        # é›¶æ–¹å·®æ£€æŸ¥ / Zero variance check
        zero_var_cols = []
        for col in feature_cols:
            if self.df[col].std() == 0:
                zero_var_cols.append(col)
        
        if zero_var_cols:
            self.warnings.append(f"é›¶æ–¹å·®åˆ—: {zero_var_cols}")
            logger.warning(f"  âš ï¸ é›¶æ–¹å·®åˆ— (å¸¸æ•°): {zero_var_cols}")
        else:
            logger.info(f"  âœ… æ— é›¶æ–¹å·®åˆ—")
    
    def validate_labels(self):
        """éªŒè¯æ ‡ç­¾åˆ†å¸ƒ / Validate label distribution"""
        logger.info("\nğŸ·ï¸ æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ...")
        
        if 'label' not in self.df.columns:
            return
        
        label_counts = self.df['label'].value_counts().sort_index()
        total = len(self.df)
        
        logger.info("  æ ‡ç­¾åˆ†å¸ƒ:")
        label_names = ['äºæŸ/Loss', 'å¾®åˆ©/Minor', 'è‰¯å¥½/Good', 'ä¼˜ç§€/Excellent']
        
        for label, count in label_counts.items():
            pct = count / total * 100
            name = label_names[label] if 0 <= label < 4 else f'Class {label}'
            logger.info(f"    {label} ({name}): {count:5d} ({pct:5.1f}%)")
        
        # æ£€æŸ¥ç±»åˆ«æ˜¯å¦å®Œæ•´ / Check class completeness
        expected_labels = {0, 1, 2, 3}
        actual_labels = set(label_counts.index)
        missing_labels = expected_labels - actual_labels
        
        if missing_labels:
            self.warnings.append(f"ç¼ºå°‘ç±»åˆ«: {missing_labels}")
            logger.warning(f"  âš ï¸ ç¼ºå°‘ç±»åˆ«: {missing_labels}")
        else:
            logger.info(f"  âœ… æ‰€æœ‰4ä¸ªç±»åˆ«éƒ½å­˜åœ¨")
        
        # æ£€æŸ¥ä¸¥é‡ä¸å¹³è¡¡ / Check severe imbalance
        min_pct = label_counts.min() / total * 100
        if min_pct < 5:
            self.warnings.append(f"ä¸¥é‡ç±»åˆ«ä¸å¹³è¡¡: æœ€å°ç±»åˆ«ä»…å  {min_pct:.1f}%")
            logger.warning(f"  âš ï¸ ä¸¥é‡ç±»åˆ«ä¸å¹³è¡¡: æœ€å°ç±»åˆ«ä»…å  {min_pct:.1f}%")
    
    def validate_statistics(self):
        """ç»Ÿè®¡ä¿¡æ¯ / Statistics"""
        logger.info("\nğŸ“ˆ ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯...")
        
        feature_cols = [c for c in FeatureExtractor.FEATURE_NAMES if c in self.df.columns]
        stats = self.df[feature_cols].describe().T
        
        # åªæ˜¾ç¤ºå…³é”®ç»Ÿè®¡ / Show key stats only
        logger.info(f"  ç‰¹å¾èŒƒå›´é¢„è§ˆ (å‰5ä¸ª):")
        for col in feature_cols[:5]:
            min_val = self.df[col].min()
            max_val = self.df[col].max()
            mean_val = self.df[col].mean()
            logger.info(f"    {col}: [{min_val:.4f}, {max_val:.4f}], mean={mean_val:.4f}")
    
    def run_full_validation(self) -> dict:
        """è¿è¡Œå®Œæ•´éªŒè¯ / Run full validation"""
        logger.info("=" * 60)
        logger.info("ğŸ”¬ ButterQuant è®­ç»ƒæ•°æ®éªŒè¯ / Training Data Validation")
        logger.info("=" * 60)
        
        if not self.load_data():
            return {'status': 'error', 'message': 'æ•°æ®åŠ è½½å¤±è´¥'}
        
        self.validate_shape()
        self.validate_features()
        self.validate_values()
        self.validate_labels()
        self.validate_statistics()
        
        # æ±‡æ€»æŠ¥å‘Š / Summary report
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“‹ éªŒè¯æŠ¥å‘Šæ±‡æ€» / Validation Summary")
        logger.info("=" * 60)
        
        if self.issues:
            logger.error(f"\nâŒ å‘ç° {len(self.issues)} ä¸ªé”™è¯¯:")
            for i, issue in enumerate(self.issues, 1):
                logger.error(f"  {i}. {issue}")
        
        if self.warnings:
            logger.warning(f"\nâš ï¸ å‘ç° {len(self.warnings)} ä¸ªè­¦å‘Š:")
            for i, warning in enumerate(self.warnings, 1):
                logger.warning(f"  {i}. {warning}")
        
        if not self.issues and not self.warnings:
            logger.info("\nâœ… æ•°æ®éªŒè¯é€šè¿‡! æœªå‘ç°ä»»ä½•é—®é¢˜ã€‚")
            status = 'pass'
        elif self.issues:
            logger.error("\nâŒ æ•°æ®éªŒè¯å¤±è´¥! è¯·ä¿®å¤ä¸Šè¿°é”™è¯¯ã€‚")
            status = 'fail'
        else:
            logger.warning("\nâš ï¸ æ•°æ®éªŒè¯é€šè¿‡, ä½†æœ‰è­¦å‘Šéœ€è¦æ³¨æ„ã€‚")
            status = 'pass_with_warnings'
        
        return {
            'status': status,
            'n_samples': len(self.df),
            'n_features': len([c for c in FeatureExtractor.FEATURE_NAMES if c in self.df.columns]),
            'issues': self.issues,
            'warnings': self.warnings
        }


def main():
    parser = argparse.ArgumentParser(description='éªŒè¯ButterQuantè®­ç»ƒæ•°æ®è´¨é‡')
    parser.add_argument('--file', type=str, default='ml/training_data_deep.parquet',
                        help='æ•°æ®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: ml/training_data_deep.parquet)')
    args = parser.parse_args()
    
    validator = DataValidator(args.file)
    result = validator.run_full_validation()
    
    # è¿”å›çŠ¶æ€ç  / Return status code
    if result['status'] == 'fail':
        sys.exit(1)
    sys.exit(0)


if __name__ == "__main__":
    main()
