# -*- coding: utf-8 -*-
"""
ButterQuant è®­ç»ƒæ•°æ®åˆå¹¶è„šæœ¬ / Training Data Merge Script
åˆå¹¶å†å²æ¨¡æ‹Ÿæ•°æ®å’Œæ•°æ®åº“å¯¼å‡ºæ•°æ® / Merge historical simulation data and database exports

ç”¨æ³• / Usage:
    python ml/merge_training_data.py
    python ml/merge_training_data.py --output merged_data.parquet
"""

import sys
import pandas as pd
import numpy as np
from pathlib import Path
import logging
import argparse
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„ / Add project path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from ml.features import FeatureExtractor

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class DataMerger:
    """è®­ç»ƒæ•°æ®åˆå¹¶å™¨ / Training Data Merger"""
    
    def __init__(self, output_dir: str = None):
        if output_dir is None:
            output_dir = PROJECT_ROOT / 'ml'
        self.output_dir = Path(output_dir)
    
    def find_data_files(self) -> list:
        """æŸ¥æ‰¾æ‰€æœ‰è®­ç»ƒæ•°æ®æ–‡ä»¶ / Find all training data files"""
        data_files = []
        
        patterns = [
            'training_data_deep.parquet',       # å†å²æ¨¡æ‹Ÿ / Historical simulation
            'training_data_from_db.parquet',    # æ•°æ®åº“å¯¼å‡º / DB export
            'training_data_*.parquet'           # å…¶ä»–åŒ¹é… / Other matches
        ]
        
        for pattern in patterns:
            for f in self.output_dir.glob(pattern):
                if f not in data_files and 'merged' not in f.name:
                    data_files.append(f)
        
        return data_files
    
    def merge_all(self, output_name: str = None) -> pd.DataFrame:
        """åˆå¹¶æ‰€æœ‰æ•°æ®æ–‡ä»¶ / Merge all data files"""
        
        data_files = self.find_data_files()
        
        if not data_files:
            logger.error("âŒ æœªæ‰¾åˆ°ä»»ä½•è®­ç»ƒæ•°æ®æ–‡ä»¶")
            return pd.DataFrame()
        
        logger.info("ğŸ“ æ‰¾åˆ°ä»¥ä¸‹æ•°æ®æ–‡ä»¶:")
        for f in data_files:
            logger.info(f"  - {f.name}")
        
        # è¯»å–å¹¶åˆå¹¶ / Read and merge
        dfs = []
        for f in data_files:
            try:
                df = pd.read_parquet(f)
                df['_source_file'] = f.name
                dfs.append(df)
                logger.info(f"  âœ… {f.name}: {len(df)} è¡Œ")
            except Exception as e:
                logger.warning(f"  âš ï¸ {f.name}: è¯»å–å¤±è´¥ ({e})")
        
        if not dfs:
            logger.error("âŒ æ²¡æœ‰å¯åˆå¹¶çš„æ•°æ®")
            return pd.DataFrame()
        
        # åˆå¹¶ / Merge
        merged = pd.concat(dfs, ignore_index=True)
        logger.info(f"\nğŸ“Š åˆå¹¶åæ€»è¡Œæ•°: {len(merged)}")
        
        # å»é‡ / Deduplicate
        feature_cols = FeatureExtractor.FEATURE_NAMES
        before_dedup = len(merged)
        
        # åŸºäºç‰¹å¾å’Œæ ‡ç­¾å»é‡ / Deduplicate based on features and label
        dedup_cols = feature_cols + ['label']
        existing_cols = [c for c in dedup_cols if c in merged.columns]
        merged = merged.drop_duplicates(subset=existing_cols, keep='last')
        
        after_dedup = len(merged)
        if before_dedup != after_dedup:
            logger.info(f"  å»é‡: {before_dedup} â†’ {after_dedup} (ç§»é™¤ {before_dedup - after_dedup} é‡å¤è¡Œ)")
        
        # éªŒè¯ç‰¹å¾å®Œæ•´æ€§ / Validate feature completeness
        missing_features = [f for f in FeatureExtractor.FEATURE_NAMES if f not in merged.columns]
        if missing_features:
            logger.warning(f"âš ï¸ ç¼ºå°‘ç‰¹å¾: {missing_features}")
            for f in missing_features:
                merged[f] = 0.0
        
        # ç»Ÿè®¡ / Statistics
        logger.info(f"\nğŸ“Š åˆå¹¶æ•°æ®ç»Ÿè®¡:")
        logger.info(f"  æ€»æ ·æœ¬æ•°: {len(merged)}")
        
        if '_source_file' in merged.columns:
            source_dist = merged['_source_file'].value_counts()
            logger.info(f"  æ¥æºåˆ†å¸ƒ:")
            for src, count in source_dist.items():
                logger.info(f"    - {src}: {count}")
        
        if 'label' in merged.columns:
            label_dist = merged['label'].value_counts().sort_index()
            logger.info(f"  æ ‡ç­¾åˆ†å¸ƒ:")
            label_names = ['äºæŸ', 'å¾®åˆ©', 'è‰¯å¥½', 'ä¼˜ç§€']
            for label, count in label_dist.items():
                pct = count / len(merged) * 100
                name = label_names[int(label)] if 0 <= label < 4 else f'Class {label}'
                logger.info(f"    {int(label)} ({name}): {count} ({pct:.1f}%)")
        
        # ä¿å­˜ / Save
        if output_name is None:
            timestamp = datetime.now().strftime('%Y%m%d')
            output_name = f'training_data_merged_{timestamp}.parquet'
        
        output_path = self.output_dir / output_name
        merged.to_parquet(output_path, index=False)
        logger.info(f"\nğŸ’¾ å·²ä¿å­˜: {output_path}")
        
        # åŒæ—¶æ›´æ–°é»˜è®¤è®­ç»ƒæ•°æ® / Also update default training data
        default_path = self.output_dir / 'training_data_deep.parquet'
        merged.to_parquet(default_path, index=False)
        logger.info(f"ğŸ’¾ å·²æ›´æ–°é»˜è®¤æ•°æ®: {default_path}")
        
        return merged


def main():
    parser = argparse.ArgumentParser(description='åˆå¹¶ButterQuantè®­ç»ƒæ•°æ®')
    parser.add_argument('--output', type=str, help='è¾“å‡ºæ–‡ä»¶å')
    args = parser.parse_args()
    
    merger = DataMerger()
    df = merger.merge_all(output_name=args.output)
    
    if len(df) > 0:
        logger.info("\nâœ… æ•°æ®åˆå¹¶å®Œæˆ!")
        logger.info("ä¸‹ä¸€æ­¥: python ml/train_model.py")


if __name__ == "__main__":
    main()
