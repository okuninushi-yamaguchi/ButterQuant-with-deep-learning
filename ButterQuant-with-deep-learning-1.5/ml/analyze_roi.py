"""
ä¿®æ­£æ ‡æ³¨é€»è¾‘ - åŸºäºå®é™…ROIåˆ†å¸ƒè°ƒæ•´é˜ˆå€¼

æ­¥éª¤:
1. åˆ†æç°æœ‰æ•°æ®çš„ROIåˆ†å¸ƒ
2. é‡æ–°å®šä¹‰æ›´åˆç†çš„é˜ˆå€¼
3. é‡æ–°æ ‡æ³¨æ•°æ®
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

def analyze_roi_distribution(parquet_path):
    """åˆ†æROIåˆ†å¸ƒï¼Œæ‰¾åˆ°åˆç†çš„é˜ˆå€¼"""
    
    df = pd.read_parquet(parquet_path)
    
    # å‡è®¾æœ‰ _debug_roi å­—æ®µ (å¦‚æœæ²¡æœ‰éœ€è¦é‡æ–°è®¡ç®—)
    if '_debug_roi' not in df.columns:
        print("âš ï¸ æ•°æ®ä¸­æ²¡æœ‰ROIå­—æ®µï¼Œéœ€è¦é‡æ–°ç”Ÿæˆ")
        return None
    
    roi = df['_debug_roi'].values
    
    print("=" * 60)
    print("ROIåˆ†å¸ƒåˆ†æ")
    print("=" * 60)
    
    # åŸºç¡€ç»Ÿè®¡
    print(f"\nåŸºç¡€ç»Ÿè®¡:")
    print(f"  æ ·æœ¬æ•°: {len(roi)}")
    print(f"  å‡å€¼: {np.mean(roi):.2%}")
    print(f"  ä¸­ä½æ•°: {np.median(roi):.2%}")
    print(f"  æ ‡å‡†å·®: {np.std(roi):.2%}")
    print(f"  æœ€å°å€¼: {np.min(roi):.2%}")
    print(f"  æœ€å¤§å€¼: {np.max(roi):.2%}")
    
    # åˆ†ä½æ•°
    print(f"\nåˆ†ä½æ•°:")
    for p in [10, 25, 50, 75, 90, 95, 99]:
        val = np.percentile(roi, p)
        print(f"  P{p}: {val:.2%}")
    
    # å½“å‰æ ‡æ³¨ç»Ÿè®¡
    current_labels = df['label'].value_counts().sort_index()
    print(f"\nå½“å‰æ ‡ç­¾åˆ†å¸ƒ:")
    for label, count in current_labels.items():
        pct = count / len(df) * 100
        print(f"  Class {label}: {count} ({pct:.1f}%)")
    
    # ç»˜åˆ¶åˆ†å¸ƒå›¾
    plt.figure(figsize=(12, 4))
    
    # ç›´æ–¹å›¾
    plt.subplot(1, 2, 1)
    plt.hist(roi, bins=50, edgecolor='black', alpha=0.7)
    plt.axvline(0, color='red', linestyle='--', label='ROI=0')
    plt.axvline(0.10, color='orange', linestyle='--', label='ROI=10%')
    plt.axvline(0.30, color='green', linestyle='--', label='ROI=30%')
    plt.xlabel('ROI')
    plt.ylabel('Frequency')
    plt.title('ROI Distribution')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # ç´¯ç§¯åˆ†å¸ƒ
    plt.subplot(1, 2, 2)
    sorted_roi = np.sort(roi)
    cumulative = np.arange(1, len(sorted_roi) + 1) / len(sorted_roi)
    plt.plot(sorted_roi, cumulative, linewidth=2)
    plt.axvline(0, color='red', linestyle='--', alpha=0.5)
    plt.axvline(0.10, color='orange', linestyle='--', alpha=0.5)
    plt.axvline(0.30, color='green', linestyle='--', alpha=0.5)
    plt.xlabel('ROI')
    plt.ylabel('Cumulative Probability')
    plt.title('Cumulative ROI Distribution')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('ml/roi_distribution_analysis.png', dpi=150)
    print("\nğŸ“Š åˆ†å¸ƒå›¾å·²ä¿å­˜: ml/roi_distribution_analysis.png")
    
    # æ¨èé˜ˆå€¼
    print("\n" + "=" * 60)
    print("æ¨èçš„æ–°é˜ˆå€¼ (åŸºäºåˆ†ä½æ•°):")
    print("=" * 60)
    
    p25 = np.percentile(roi, 25)
    p50 = np.percentile(roi, 50)
    p75 = np.percentile(roi, 75)
    
    print(f"\næ–¹æ¡ˆA (å››åˆ†ä½æ•°æ³•):")
    print(f"  äºæŸ:  ROI < {p25:.2%}")
    print(f"  å¾®åˆ©:  {p25:.2%} â‰¤ ROI < {p50:.2%}")
    print(f"  è‰¯å¥½:  {p50:.2%} â‰¤ ROI < {p75:.2%}")
    print(f"  ä¼˜ç§€:  ROI â‰¥ {p75:.2%}")
    print(f"  é¢„æœŸåˆ†å¸ƒ: 25% / 25% / 25% / 25%")
    
    # æ›´å®ç”¨çš„æ–¹æ¡ˆ
    t1 = -0.05  # äºæŸè¶…è¿‡5%
    t2 = 0.10   # ç›ˆåˆ©10%
    t3 = 0.25   # ç›ˆåˆ©25%
    
    print(f"\næ–¹æ¡ˆB (å®ç”¨é˜ˆå€¼æ³•):")
    print(f"  äºæŸ:  ROI < {t1:.0%}")
    print(f"  å¾®åˆ©:  {t1:.0%} â‰¤ ROI < {t2:.0%}")
    print(f"  è‰¯å¥½:  {t2:.0%} â‰¤ ROI < {t3:.0%}")
    print(f"  ä¼˜ç§€:  ROI â‰¥ {t3:.0%}")
    
    c0 = (roi < t1).sum()
    c1 = ((roi >= t1) & (roi < t2)).sum()
    c2 = ((roi >= t2) & (roi < t3)).sum()
    c3 = (roi >= t3).sum()
    
    print(f"  é¢„æœŸåˆ†å¸ƒ: {c0} / {c1} / {c2} / {c3}")
    print(f"  ç™¾åˆ†æ¯”: {c0/len(roi)*100:.1f}% / {c1/len(roi)*100:.1f}% / {c2/len(roi)*100:.1f}% / {c3/len(roi)*100:.1f}%")
    
    return {
        'method_a': (p25, p50, p75),
        'method_b': (t1, t2, t3)
    }


def relabel_dataset(parquet_path, thresholds, output_path=None):
    """
    ä½¿ç”¨æ–°é˜ˆå€¼é‡æ–°æ ‡æ³¨æ•°æ®
    
    å‚æ•°:
        thresholds: (t1, t2, t3) - ä¸‰ä¸ªé˜ˆå€¼
    """
    df = pd.read_parquet(parquet_path)
    
    if '_debug_roi' not in df.columns:
        print("âŒ ç¼ºå°‘ROIå­—æ®µï¼Œæ— æ³•é‡æ–°æ ‡æ³¨")
        return
    
    roi = df['_debug_roi'].values
    t1, t2, t3 = thresholds
    
    # é‡æ–°æ ‡æ³¨
    new_labels = np.zeros(len(roi), dtype=int)
    new_labels[roi < t1] = 0
    new_labels[(roi >= t1) & (roi < t2)] = 1
    new_labels[(roi >= t2) & (roi < t3)] = 2
    new_labels[roi >= t3] = 3
    
    df['label'] = new_labels
    
    # ç»Ÿè®¡
    print("\n" + "=" * 60)
    print("é‡æ–°æ ‡æ³¨ç»“æœ")
    print("=" * 60)
    
    old_dist = df['label'].value_counts().sort_index()
    print("\næ–°çš„æ ‡ç­¾åˆ†å¸ƒ:")
    for label, count in old_dist.items():
        pct = count / len(df) * 100
        print(f"  Class {label}: {count} ({pct:.1f}%)")
    
    # ä¿å­˜
    if output_path is None:
        pass # output_path = parquet_path (overwrite)
    
    # ä¸ºäº†å®‰å…¨ï¼Œè¿™é‡Œè¦†ç›–åŸæ–‡ä»¶ï¼Œä½†å»ºè®®å…ˆå¤‡ä»½
    # user wants to train on this, so overwriting is the standard way or pointing train_model to new file.
    # The prompt says "Please use new data file to retrain".
    # I'll overwrite 'training_data_deep.parquet' directly if output_path is None, 
    # OR I'll save to 'training_data_deep_relabeled.parquet' and update train script?
    # Claude's script said "relabel_dataset(parquet_path, thresholds)" which defaults to _relabeled.parquet.
    # But then said "Please use new data file to retrain". 
    # I will modify main to default overwrite or output to _relabeled and print instructions.
    
    if output_path:
        save_path = output_path
    else:
        save_path = str(parquet_path).replace('.parquet', '_relabeled.parquet')

    df.to_parquet(save_path, index=False)
    print(f"\nâœ… å·²ä¿å­˜åˆ°: {save_path}")
    
    return save_path


if __name__ == "__main__":
    parquet_path = "ml/training_data_deep.parquet"
    
    if not Path(parquet_path).exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {parquet_path}")
        exit()

    # 1. åˆ†æåˆ†å¸ƒ
    thresholds_dict = analyze_roi_distribution(parquet_path)
    
    if thresholds_dict is None:
        exit()

    # 2. è‡ªåŠ¨é€‰æ‹©æ–¹æ¡ˆB (å®ç”¨é˜ˆå€¼æ³•) - å› ä¸ºè¿™æ˜¯å…¨è‡ªåŠ¨è„šæœ¬
    print("\n" + "=" * 60)
    print("è‡ªåŠ¨é€‰æ‹©æ–¹æ¡ˆB (å®ç”¨é˜ˆå€¼æ³•) è¿›è¡Œé‡æ ‡æ³¨ ...")
    print("=" * 60)
    
    thresholds = thresholds_dict['method_b']
    
    # é‡æ–°æ ‡æ³¨å¹¶è¦†ç›–/ä¿å­˜
    # æ—¢ç„¶æ˜¯å…¨è‡ªåŠ¨æµç¨‹ï¼Œä¸ºäº†æ–¹ä¾¿å¯ä»¥ç›´æ¥è¦†ç›–ï¼Œæˆ–è€…ä¿å­˜ä¸ºæ–°æ–‡ä»¶ç„¶åæˆ‘æ”¹è®­ç»ƒè„šæœ¬è·¯å¾„ã€‚
    # æ¯”è¾ƒç¨³å¦¥çš„æ˜¯ä¿å­˜ä¸ºæ–°æ–‡ä»¶ï¼Œç„¶åæ”¹è®­ç»ƒè„šæœ¬è¯»å–è¿™ä¸ªæ–°æ–‡ä»¶ã€‚
    
    new_path = relabel_dataset(parquet_path, thresholds)
    
    print(f"\nâœ… æ•°æ®å·²å‡†å¤‡å¥½: {new_path}")
    print("å»ºè®®ä¿®æ”¹ ml/train_model.py è¯»å–æ­¤æ–°æ–‡ä»¶è¿›è¡Œè®­ç»ƒã€‚")
