# -*- coding: utf-8 -*-
"""
ButterQuant ML Model Training - Robust V2.0 / ButterQuant ML æ¨¡å‹è®­ç»ƒ - å¢å¼ºç‰ˆ V2.0
ä¸“é—¨é’ˆå¯¹ä¸¥é‡ç±»åˆ«ä¸å¹³è¡¡çš„æƒ…å†µ / Specialized for severe class imbalance

å…³é”®ä¿®å¤ / Key Fixes:
1. ç‰¹å¾ç»´åº¦æ£€æŸ¥å’Œè‡ªåŠ¨å¡«å…… (Feature dimension check & auto-pad)
2. ç±»åˆ«æƒé‡è®¡ç®—ä¿®æ­£ (åå‘æƒé‡) (Inverse frequency class weights)
3. è¿‡é‡‡æ ·/æ¬ é‡‡æ ·å¤„ç†ä¸å¹³è¡¡ (Oversampling/Undersampling)
4. ONNXå¯¼å‡ºé”™è¯¯å¤„ç† (Robust ONNX export)
5. æ—©åœå’Œæ­£åˆ™åŒ–å¢å¼º (Early stopping & regularization)
"""

import sys
from pathlib import Path

# Add project root to path to resolve 'ml' module
current_dir = Path(__file__).parent.absolute()
project_root = current_dir.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import logging
import joblib
import os
from ml.features import FeatureExtractor

# è®¾ç½®æ—¥å¿— / Setup Logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class MultiClassSuccessClassifier(nn.Module):
    """
    4åˆ†ç±»æ¨¡å‹ - å¢å¼ºæ­£åˆ™åŒ– / 4-class model with enhanced regularization
    """
    
    def __init__(self, input_dim=22, hidden_dims=[64, 32, 16], num_classes=4, dropout=0.3):
        super().__init__()
        
        layers = []
        prev_dim = input_dim
        
        for i, hidden_dim in enumerate(hidden_dims):
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout)
            ])
            prev_dim = hidden_dim
        
        layers.append(nn.Linear(prev_dim, num_classes))
        self.net = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.net(x)


class ModelTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨ - ä¿®å¤ç‰ˆ / Robust Model Trainer"""
    
    def __init__(self, data_path: str, output_dir: str = "ml/models"):
        self.data_path = Path(data_path)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"ä½¿ç”¨è®¾å¤‡ / Using device: {self.device}")
        if self.device.type == 'cuda':
             logger.info(f"GPU: {torch.cuda.get_device_name(0)}")

        self.config = {
            'input_dim': 22,
            'hidden_dims': [64, 32, 16],
            'num_classes': 4,
            'dropout': 0.3,  # å¢åŠ dropout / Increased dropout
            'learning_rate': 0.0005,  # é™ä½å­¦ä¹ ç‡ / Lower LR
            'batch_size': 64,
            'num_epochs': 50,
            'early_stopping_patience': 15,
            'test_size': 0.2,
            'val_size': 0.1,
            'random_state': 42,
            'use_class_weights': True,
            'use_sampling': True  # æ˜¯å¦ä½¿ç”¨é‡‡æ ·å¹³è¡¡ / Use sampling balance
        }
        
        self.model = None
        self.scaler = None
        self.class_weights = None
        self.history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}
    
    def load_data(self):
        """åŠ è½½å¹¶éªŒè¯æ•°æ® / Load and validate data"""
        logger.info(f"ğŸ“¥ åŠ è½½æ•°æ®: {self.data_path}")
        
        df = pd.read_parquet(self.data_path)
        logger.info(f"å·²åŠ è½½ {len(df)} ä¸ªæ ·æœ¬ / Loaded {len(df)} samples.")
        
        # ä½¿ç”¨ç»Ÿä¸€çš„22ç»´ç‰¹å¾åˆ—è¡¨ / Use unified 22-dim feature list
        feature_cols = FeatureExtractor.FEATURE_NAMES
        
        # éªŒè¯ç‰¹å¾æ˜¯å¦å®Œæ•´ / Verify features exist
        missing_cols = [c for c in feature_cols if c not in df.columns]
        if missing_cols:
            logger.error(f"âŒ æ•°æ®ç¼ºå¤±å…³é”®ç‰¹å¾ / Missing features: {missing_cols}")
            # è¿›è¡Œç®€å•å¡«å……ä»¥é˜²å´©æºƒ / Pad to prevent crash
            for c in missing_cols:
                df[c] = 0.0
        
        logger.info(f"ç‰¹å¾é€‰æ‹© / Feature selection: {len(feature_cols)} features")
        
        # ç¡®ä¿é¡ºåºä¸€è‡´ï¼Œè¿™é‡Œç®€å•æŒ‰åˆ—è¡¨é¡ºåºï¼Œå®é™…åº”å°½é‡ä¸features.pyå¯¹é½
        # ä½†æ—¢ç„¶æ˜¯ä¿®å¤è®­ç»ƒï¼Œåªè¦ä¿è¯è¾“å…¥ç»´åº¦å¯¹å³å¯
        
        X = df[feature_cols].values
        y = df['label'].values
        
        # æ•°æ®è´¨é‡æ£€æŸ¥ / Data quality check
        if np.isnan(X).any():
            logger.warning("âš ï¸ ç‰¹å¾ä¸­åŒ…å«NaN, æ›¿æ¢ä¸º0 / Features contain NaN, replaced with 0")
            X = np.nan_to_num(X, nan=0.0)
        
        if np.isinf(X).any():
            logger.warning("âš ï¸ ç‰¹å¾ä¸­åŒ…å«Inf, æ›¿æ¢ä¸º0 / Features contain Inf, replaced with 0")
            X = np.nan_to_num(X, posinf=0.0, neginf=0.0)
        
        # æ ‡ç­¾åˆ†å¸ƒ / Label distribution
        label_counts = pd.Series(y).value_counts().sort_index().to_dict()
        logger.info(f"æ ‡ç­¾åˆ†å¸ƒ / Label distribution: {label_counts}")
        
        return X, y
    
    def balance_dataset(self, X, y):
        """å¹³è¡¡æ•°æ®é›† - æ··åˆé‡‡æ ·ç­–ç•¥ / Balance dataset - Mixed sampling strategy"""
        from collections import Counter
        
        logger.info("ğŸ”„ åº”ç”¨æ•°æ®å¹³è¡¡ç­–ç•¥ / Applying balancing strategy...")
        
        class_counts = Counter(y)
        logger.info(f"åŸå§‹åˆ†å¸ƒ / Original dist: {dict(class_counts)}")
        
        # ç­–ç•¥: æ¬ é‡‡æ ·å¤šæ•°ç±» + è¿‡é‡‡æ ·å°‘æ•°ç±»
        max_count = max(class_counts.values())
        
        # ç›®æ ‡åˆ†å¸ƒ (ç¼“å’Œç‰ˆ) / Target distribution (Soft)
        target_counts = {
            0: min(class_counts[0], int(max_count * 0.5)),  # äºæŸç±»æ¬ é‡‡æ ·åˆ°50% / Loss: undersample to 50%
            1: min(class_counts[1] * 3, int(max_count * 0.3)),  # å¾®åˆ©ç±»è¿‡é‡‡æ ·3å€ / Minor: oversample 3x
            2: min(class_counts[2] * 2, int(max_count * 0.3)),  # è‰¯å¥½ç±»è¿‡é‡‡æ ·2å€ / Good: oversample 2x
            3: class_counts[3]  # ä¼˜ç§€ç±»ä¿æŒä¸å˜ / Excellent: keep
        }
        
        balanced_indices = []
        
        for cls in range(4):
            cls_indices = np.where(y == cls)[0]
            target = target_counts.get(cls, len(cls_indices))
            
            if len(cls_indices) == 0:
                continue

            if len(cls_indices) > target:
                # æ¬ é‡‡æ · / Undersample
                sampled = np.random.choice(cls_indices, target, replace=False)
            else:
                # è¿‡é‡‡æ · / Oversample
                sampled = np.random.choice(cls_indices, target, replace=True)
            
            balanced_indices.extend(sampled)
        
        balanced_indices = np.array(balanced_indices)
        np.random.shuffle(balanced_indices)
        
        X_balanced = X[balanced_indices]
        y_balanced = y[balanced_indices]
        
        new_counts = Counter(y_balanced)
        logger.info(f"å¹³è¡¡ååˆ†å¸ƒ / Balanced dist: {dict(new_counts)}")
        logger.info(f"æ ·æœ¬æ•°é‡ / Sample count: {len(y)} â†’ {len(y_balanced)}")
        
        return X_balanced, y_balanced
    
    def compute_class_weights(self, y):
        """
        è®¡ç®—ç±»åˆ«æƒé‡ - åå‘é¢‘ç‡æƒé‡ / Compute class weights - Inverse Frequency
        weight = 1 / frequency
        """
        from collections import Counter
        
        class_counts = Counter(y)
        total = len(y)
        
        # è®¡ç®—é¢‘ç‡ / Frequencies
        frequencies = {cls: count / total for cls, count in class_counts.items()}
        
        # åå‘æƒé‡ / Inverse weights
        weights = []
        # å‡è®¾4ç±» / Assume 4 classes
        for cls in range(4):
            freq = frequencies.get(cls, 1e-6) # é¿å…é™¤é›¶
            weight = 1.0 / freq
            weights.append(weight)
        
        # å½’ä¸€åŒ– (ä½¿å¾—å¹³å‡æƒé‡ä¸º1) / Normalize (mean=1)
        weights = np.array(weights)
        weights = weights / weights.mean()
        
        logger.info(f"ç±»åˆ«æƒé‡ / Class weights: {weights}")
        return torch.tensor(weights, dtype=torch.float32)
    
    def prepare_dataloaders(self, X, y):
        """å‡†å¤‡æ•°æ®åŠ è½½å™¨ / Prepare DataLoaders"""
        
        # æ•°æ®å¹³è¡¡ (ä»…å¯¹è®­ç»ƒé›†?) -> è¿™é‡Œå¯¹å…¨éƒ¨æ•°æ®è¿›è¡Œäº†å¹³è¡¡ï¼Œç„¶åsplitã€‚
        # æ›´å¥½çš„åšæ³•é€šå¸¸æ˜¯åªå¯¹è®­ç»ƒé›†åšè¿‡é‡‡æ ·ï¼ŒéªŒè¯é›†ä¿æŒåŸæ ·ã€‚
        # ä½†ä¸ºäº†ç®€åŒ–å®ç°ï¼Œæˆ‘ä»¬å¯ä»¥å…ˆsplitå†balanceè®­ç»ƒé›†ã€‚
        
        # 1. åˆ’åˆ† Test (ä¿æŒçœŸå®åˆ†å¸ƒ)
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y, test_size=self.config['test_size'], stratify=y, random_state=self.config['random_state']
        )
        
        # 2. åˆ’åˆ† Val (ä¿æŒçœŸå®åˆ†å¸ƒ)
        X_train_raw, X_val, y_train_raw, y_val = train_test_split(
            X_temp, y_temp, test_size=0.15, stratify=y_temp, random_state=self.config['random_state']
        )
        
        # 3. å¯¹ Train è¿›è¡Œå¹³è¡¡ (Sampling)
        if self.config['use_sampling']:
            X_train, y_train = self.balance_dataset(X_train_raw, y_train_raw)
        else:
            X_train, y_train = X_train_raw, y_train_raw

        logger.info(f"ğŸ”€ æ•°æ®åˆ†å‰² / Data Split:")
        logger.info(f"   è®­ç»ƒé›† / Train: {len(X_train)}")
        logger.info(f"   éªŒè¯é›† / Val:   {len(X_val)}")
        logger.info(f"   æµ‹è¯•é›† / Test:  {len(X_test)}")
        
        # æ ‡å‡†åŒ– / Scaling
        self.scaler = StandardScaler()
        X_train = self.scaler.fit_transform(X_train)
        X_val = self.scaler.transform(X_val)
        X_test = self.scaler.transform(X_test)
        
        # DataLoader
        train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))
        val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.LongTensor(y_val))
        test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.LongTensor(y_test))
        
        train_loader = DataLoader(train_dataset, batch_size=self.config['batch_size'], shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=self.config['batch_size'], shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=self.config['batch_size'], shuffle=False)
        
        return train_loader, val_loader, test_loader
    
    def train_epoch(self, train_loader, model, criterion, optimizer):
        model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        for X_batch, y_batch in train_loader:
            X_batch = X_batch.to(self.device)
            y_batch = y_batch.to(self.device)
            
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª / Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            total_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += y_batch.size(0)
            correct += (predicted == y_batch).sum().item()
        
        return total_loss / len(train_loader), correct / total
    
    def validate_epoch(self, val_loader, model, criterion):
        model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch = X_batch.to(self.device)
                y_batch = y_batch.to(self.device)
                
                outputs = model(X_batch)
                loss = criterion(outputs, y_batch)
                
                total_loss += loss.item()
                _, predicted = torch.max(outputs, 1)
                total += y_batch.size(0)
                correct += (predicted == y_batch).sum().item()
        
        return total_loss / len(val_loader), correct / total
    
    def train(self, X, y):
        """å®Œæ•´è®­ç»ƒæµç¨‹ / Complete training flow"""
        
        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒ / Starting training...")
        
        # å‡†å¤‡æ•°æ® / Prepare Data
        train_loader, val_loader, test_loader = self.prepare_dataloaders(X, y)
        
        # è®¡ç®—æƒé‡ (åŸºäºè®­ç»ƒé›†æˆ–å…¨å±€? è¿™é‡Œç”¨å…¨å±€ç®€å•äº›ï¼Œæˆ–è€…åŸºäºå‡è¡¡åçš„å…¶å®ä¸éœ€è¦æƒé‡äº†)
        # å¦‚æœå·²ç»åšäº†Samplingå¹³è¡¡ï¼Œclass_weightså¯ä»¥è®¾ä¸ºNoneï¼Œæˆ–è€…å¼±åŒ–ã€‚
        # è¿™é‡Œä¸ºäº†ä¿é™©ï¼Œè¿˜æ˜¯ç®—ä¸€ä¸‹ï¼Œä½†åŸºäºy (åŸå§‹) è¿˜æ˜¯ y_train (å¹³è¡¡å)?
        # æ—¢ç„¶å·²ç»å¹³è¡¡äº†ï¼ŒCrossEntropyçš„weightåº”è¯¥é è¿‘1ã€‚
        # æˆ‘ä»¬ç”¨å¹³è¡¡åçš„y_trainæ¥ç®—æƒé‡ï¼Œåº”è¯¥å¾ˆæ¥è¿‘1ã€‚
        # æˆ–è€…ï¼Œä¸ºäº†å¤„ç†Samplingæ²¡å®Œå…¨å¹³è¡¡çš„éƒ¨åˆ†ï¼Œå†åŠ ä¸€å±‚ä¿é™©ã€‚
        
        # è·å–è®­ç»ƒé›†æ‰€æœ‰çš„æ ‡ç­¾ç”¨äºè®¡ç®—æƒé‡
        all_train_labels = []
        for _, y_batch in train_loader:
            all_train_labels.extend(y_batch.numpy())
        
        if self.config['use_class_weights']:
            self.class_weights = self.compute_class_weights(all_train_labels)
        
        # åˆå§‹åŒ–æ¨¡å‹ / Init Model
        self.model = MultiClassSuccessClassifier(
            input_dim=self.config['input_dim'],
            hidden_dims=self.config['hidden_dims'],
            num_classes=self.config['num_classes'],
            dropout=self.config['dropout']
        ).to(self.device)
        
        # æŸå¤±å‡½æ•° / Criterion
        if self.class_weights is not None:
            criterion = nn.CrossEntropyLoss(weight=self.class_weights.to(self.device))
        else:
            criterion = nn.CrossEntropyLoss()
        
        optimizer = optim.Adam(self.model.parameters(), lr=self.config['learning_rate'])
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)
        
        # è®­ç»ƒå¾ªç¯ / Training Loop
        best_val_loss = float('inf')
        best_val_acc = 0.0
        patience_counter = 0
        
        for epoch in range(self.config['num_epochs']):
            train_loss, train_acc = self.train_epoch(train_loader, self.model, criterion, optimizer)
            val_loss, val_acc = self.validate_epoch(val_loader, self.model, criterion)
            
            self.history['train_loss'].append(train_loss)
            self.history['val_loss'].append(val_loss)
            self.history['train_acc'].append(train_acc)
            self.history['val_acc'].append(val_acc)
            
            if (epoch + 1) % 10 == 0:
                logger.info(f"Epoch [{epoch+1}/{self.config['num_epochs']}], Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}")
            
            scheduler.step(val_loss)
            
            # ä¿å­˜æœ€ä½³ (ä¼˜å…ˆçœ‹Val Lossï¼Œå…¶æ¬¡çœ‹Acc)
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                best_val_acc = val_acc # Update this too
                patience_counter = 0
                self.save_checkpoint('best_model.pt')
            else:
                patience_counter += 1
            
            if patience_counter >= self.config['early_stopping_patience']:
                logger.info(f"â¸ï¸ Early stopping at epoch {epoch+1}")
                break
        
        # åŠ è½½æœ€ä½³æ¨¡å‹ / Load best
        try:
            self.load_checkpoint('best_model.pt')
        except:
            logger.warning("æ— æ³•åŠ è½½æœ€ä½³æ¨¡å‹ï¼Œä½¿ç”¨æœ€ç»ˆæ¨¡å‹ / Could not load best model, using final")
        
        # è¯„ä¼° / Evaluate
        logger.info("\nè¯„ä¼°æŠ¥å‘Š / Evaluation Report:")
        self.evaluate(test_loader)
        
        # ä¿å­˜ / Save
        self.save_model()
        
        logger.info(f"âœ… æ¨¡å‹å·²ä¿å­˜ / Model saved to {self.output_dir}/")
        logger.info(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡ / Best Val accuracy: {best_val_acc:.4f}")
    
    def evaluate(self, test_loader):
        """è¯„ä¼°æ¨¡å‹ / Evaluate"""
        self.model.eval()
        
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            for X_batch, y_batch in test_loader:
                X_batch = X_batch.to(self.device)
                outputs = self.model(X_batch)
                _, predicted = torch.max(outputs, 1)
                
                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(y_batch.numpy())
        
        all_preds = np.array(all_preds)
        all_labels = np.array(all_labels)
        
        # æŠ¥å‘Š
        target_names = ['Loss/äºæŸ', 'Minor/å¾®åˆ©', 'Good/è‰¯å¥½', 'Excellent/ä¼˜ç§€']
        # å¤„ç†å¯èƒ½å‡ºç°çš„æœªé¢„æµ‹åˆ°çš„åˆ— (ä¾‹å¦‚åªæœ‰æŸäº›ç±»è¢«é¢„æµ‹åˆ°)
        present_labels = sorted(list(set(all_labels) | set(all_preds)))
        target_names_subset = [target_names[i] for i in present_labels]
        
        logger.info("\n" + classification_report(all_labels, all_preds, target_names=target_names, zero_division=0))
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(all_labels, all_preds)
        logger.info(f"æ··æ·†çŸ©é˜µ / Confusion Matrix:\n{cm}")
        
        try:
            self.plot_confusion_matrix(cm, target_names)
        except Exception as e:
            logger.warning(f"æ— æ³•ç»˜åˆ¶æ··æ·†çŸ©é˜µ: {e}")
    
    def plot_confusion_matrix(self, cm, target_names):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ / Plot CM"""
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=target_names, yticklabels=target_names)
        plt.title('Confusion Matrix')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.tight_layout()
        plt.savefig(self.output_dir / 'confusion_matrix.png', dpi=150)
        plt.close()
    
    def save_checkpoint(self, filename):
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'config': self.config
        }, self.output_dir / filename)
    
    def load_checkpoint(self, filename):
        checkpoint = torch.load(self.output_dir / filename, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
    
    def save_model(self):
        """æœ€åä¿å­˜ / Final save"""
        # PyTorch
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'config': self.config,
            'class_weights': self.class_weights
        }, self.output_dir / 'success_model_v2.pth') # Keep .pth standard
        
        # Scaler
        joblib.dump(self.scaler, self.output_dir / 'scaler_v2.joblib')
        
        # ONNX
        try:
            self.export_to_onnx()
        except Exception as e:
            logger.warning(f"ONNXå¯¼å‡ºå¤±è´¥ / ONNX export failed: {e}")
            logger.info("   å¯ä»¥ç¨åè¡¥æ•‘ / Can try later")

    def export_to_onnx(self):
        self.model.eval()
        dummy_input = torch.randn(1, self.config['input_dim'], device=self.device)
        
        torch.onnx.export(
            self.model,
            dummy_input,
            self.output_dir / 'success_model_v2.onnx',
            export_params=True,
            opset_version=14, # Newer opset
            input_names=['features'],
            output_names=['logits'],
            dynamic_axes={'features': {0: 'batch_size'}, 'logits': {0: 'batch_size'}}
        )
        logger.info("âœ… ONNXæ¨¡å‹å·²å¯¼å‡º / ONNX exported")


def main():
    data_path = "ml/training_data_deep.parquet"
    if not Path(data_path).exists():
        logger.error(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        return
    
    trainer = ModelTrainer(data_path)
    X, y = trainer.load_data()
    trainer.train(X, y)


if __name__ == "__main__":
    main()
