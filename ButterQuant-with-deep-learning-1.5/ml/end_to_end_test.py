# -*- coding: utf-8 -*-
"""
ButterQuant ç«¯åˆ°ç«¯æµ‹è¯•è„šæœ¬ / End-to-End Test Script
æµ‹è¯•å®Œæ•´MLæµç¨‹: æ•°æ®ç”Ÿæˆ â†’ ç‰¹å¾æå– â†’ æ¨¡å‹è®­ç»ƒ â†’ æ¨ç† / Test complete ML flow

ç”¨æ³• / Usage:
    python ml/end_to_end_test.py
    python ml/end_to_end_test.py --quick  # å¿«é€Ÿæµ‹è¯•æ¨¡å¼
"""

import sys
import os
import time
import numpy as np
import pandas as pd
from pathlib import Path
import logging
import argparse

# æ·»åŠ é¡¹ç›®è·¯å¾„ / Add project path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))
sys.path.insert(0, str(PROJECT_ROOT / 'backend'))

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class EndToEndTest:
    """ç«¯åˆ°ç«¯æµ‹è¯•å™¨ / End-to-End Tester"""
    
    def __init__(self, quick_mode: bool = False):
        self.quick_mode = quick_mode
        self.results = {}
        self.start_time = time.time()
        
    def test_feature_extractor(self) -> bool:
        """æµ‹è¯•ç‰¹å¾æå–å™¨ / Test Feature Extractor"""
        logger.info("\n" + "=" * 50)
        logger.info("ğŸ“Š æµ‹è¯•1: ç‰¹å¾æå–å™¨ / Test 1: Feature Extractor")
        logger.info("=" * 50)
        
        try:
            from ml.features import FeatureExtractor, extract_features_v2
            
            # åˆ›å»ºæ¨¡æ‹Ÿåˆ†æç»“æœ / Create mock analysis result
            mock_analysis = {
                'fourier': {
                    'trend_slope': 0.05,
                    'dominant_period_days': 21,
                    'period_strength': 0.3
                },
                'arima': {
                    'mean_forecast': 150.0,
                    'confidence_interval_width': 10.0
                },
                'garch': {
                    'predicted_vol': 0.25,
                    'current_iv': 0.30,
                    'vol_mispricing': 0.20,
                    'iv_percentile': 0.65
                },
                'greeks': {
                    'delta': 0.01,
                    'gamma': 0.05,
                    'vega': 20.0,
                    'theta': -5.0
                },
                'butterfly': {
                    'max_profit': 100,
                    'max_loss': 50,
                    'profit_ratio': 2.0,
                    'prob_profit': 0.6,
                    'dte': 30
                }
            }
            
            # æå–ç‰¹å¾ / Extract features
            features = extract_features_v2(mock_analysis)
            
            # éªŒè¯ / Validate
            assert len(features) == 23, f"ç‰¹å¾æ•°åº”ä¸º23, å®é™…: {len(features)}"
            assert all(k in features for k in FeatureExtractor.FEATURE_NAMES), "ç¼ºå°‘å¿…è¦ç‰¹å¾"
            
            # è½¬æ¢ä¸ºæ•°ç»„ / Convert to array
            arr = FeatureExtractor.to_array(features)
            assert arr.shape == (23,), f"æ•°ç»„å½¢çŠ¶åº”ä¸º(23,), å®é™…: {arr.shape}"
            assert arr.dtype == np.float32, f"æ•°æ®ç±»å‹åº”ä¸ºfloat32"
            
            logger.info(f"  âœ… ç‰¹å¾æå–æˆåŠŸ: {len(features)} ç»´")
            logger.info(f"  âœ… æ•°ç»„è½¬æ¢æˆåŠŸ: shape={arr.shape}")
            
            self.results['feature_extractor'] = True
            return True
            
        except Exception as e:
            logger.error(f"  âŒ ç‰¹å¾æå–å¤±è´¥: {e}")
            self.results['feature_extractor'] = False
            return False
    
    def test_ml_inference(self) -> bool:
        """æµ‹è¯•MLæ¨ç†å¼•æ“ / Test ML Inference Engine"""
        logger.info("\n" + "=" * 50)
        logger.info("ğŸ¤– æµ‹è¯•2: MLæ¨ç†å¼•æ“ / Test 2: ML Inference Engine")
        logger.info("=" * 50)
        
        try:
            from backend.ml_inference import ModelInference, get_inference_engine
            from ml.features import FeatureExtractor
            
            # è·å–å¼•æ“ / Get engine
            engine = ModelInference()
            version = engine.get_model_version()
            
            logger.info(f"  æ¨¡å‹ç‰ˆæœ¬ / Model version: {version or 'æœªåŠ è½½'}")
            
            if version is None:
                logger.warning("  âš ï¸ æ¨¡å‹æœªåŠ è½½ (å¯èƒ½å°šæœªè®­ç»ƒ)")
                logger.info("  â†’ è·³è¿‡æ¨ç†æµ‹è¯•, è¯·å…ˆè®­ç»ƒæ¨¡å‹")
                self.results['ml_inference'] = 'skipped'
                return True
            
            # ç”Ÿæˆéšæœºç‰¹å¾ / Generate random features
            mock_features = {
                name: float(np.random.randn()) 
                for name in FeatureExtractor.FEATURE_NAMES
            }
            
            # æ‰§è¡Œæ¨ç† / Run inference
            result = engine.predict_roi_distribution(mock_features)
            
            if result is None:
                logger.warning("  âš ï¸ æ¨ç†è¿”å›None")
                self.results['ml_inference'] = False
                return False
            
            # éªŒè¯ç»“æœ / Validate result
            required_keys = ['prob_loss', 'prob_minor', 'prob_good', 'prob_excellent', 'expected_roi']
            for key in required_keys:
                assert key in result, f"ç¼ºå°‘å­—æ®µ: {key}"
            
            # æ¦‚ç‡å’Œåº”ä¸º1 / Probabilities should sum to 1
            prob_sum = result['prob_loss'] + result['prob_minor'] + result['prob_good'] + result['prob_excellent']
            assert 0.99 <= prob_sum <= 1.01, f"æ¦‚ç‡å’Œåº”ä¸º1, å®é™…: {prob_sum}"
            
            logger.info(f"  âœ… æ¨ç†æˆåŠŸ!")
            logger.info(f"  - P(äºæŸ): {result['prob_loss']:.2%}")
            logger.info(f"  - P(å¾®åˆ©): {result['prob_minor']:.2%}")
            logger.info(f"  - P(è‰¯å¥½): {result['prob_good']:.2%}")
            logger.info(f"  - P(ä¼˜ç§€): {result['prob_excellent']:.2%}")
            logger.info(f"  - æœŸæœ›ROI: {result['expected_roi']:.2%}")
            
            # æ€§èƒ½æµ‹è¯• / Performance test
            if not self.quick_mode:
                logger.info("\n  â±ï¸ æ€§èƒ½æµ‹è¯• (100æ ·æœ¬)...")
                start = time.time()
                for _ in range(100):
                    engine.predict_roi_distribution(mock_features)
                elapsed = time.time() - start
                avg_ms = elapsed / 100 * 1000
                logger.info(f"  - å¹³å‡å»¶è¿Ÿ: {avg_ms:.2f}ms/æ ·æœ¬")
                
                if avg_ms < 2.0:
                    logger.info(f"  âœ… æ€§èƒ½è¾¾æ ‡ (<2ms)")
                else:
                    logger.warning(f"  âš ï¸ æ€§èƒ½æœªè¾¾æ ‡ (>2ms)")
            
            self.results['ml_inference'] = True
            return True
            
        except Exception as e:
            logger.error(f"  âŒ æ¨ç†æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            self.results['ml_inference'] = False
            return False
    
    def test_data_validation(self) -> bool:
        """æµ‹è¯•æ•°æ®éªŒè¯ / Test Data Validation"""
        logger.info("\n" + "=" * 50)
        logger.info("ğŸ” æµ‹è¯•3: æ•°æ®éªŒè¯å·¥å…· / Test 3: Data Validation")
        logger.info("=" * 50)
        
        try:
            from ml.validate_data import DataValidator
            from ml.features import FeatureExtractor
            
            # åˆ›å»ºæµ‹è¯•æ•°æ® / Create test data
            n_samples = 100
            test_data = {
                name: np.random.randn(n_samples)
                for name in FeatureExtractor.FEATURE_NAMES
            }
            test_data['label'] = np.random.randint(0, 4, n_samples)
            
            # ä¿å­˜ä¸´æ—¶æ–‡ä»¶ / Save temp file
            temp_path = Path(__file__).parent / '_temp_test_data.parquet'
            df = pd.DataFrame(test_data)
            df.to_parquet(temp_path)
            
            # è¿è¡ŒéªŒè¯ / Run validation
            validator = DataValidator(str(temp_path))
            result = validator.run_full_validation()
            
            # æ¸…ç† / Cleanup
            temp_path.unlink()
            
            logger.info(f"\n  éªŒè¯ç»“æœ: {result['status']}")
            
            self.results['data_validation'] = True
            return True
            
        except Exception as e:
            logger.error(f"  âŒ æ•°æ®éªŒè¯æµ‹è¯•å¤±è´¥: {e}")
            self.results['data_validation'] = False
            return False
    
    def test_execution_engine_integration(self) -> bool:
        """æµ‹è¯•æ‰§è¡Œå¼•æ“é›†æˆ / Test Execution Engine Integration"""
        logger.info("\n" + "=" * 50)
        logger.info("ğŸš€ æµ‹è¯•4: æ‰§è¡Œå¼•æ“é›†æˆ / Test 4: Execution Engine Integration")
        logger.info("=" * 50)
        
        try:
            # åªæµ‹è¯•å¯¼å…¥å’Œåˆå§‹åŒ–,ä¸å®é™…è¿æ¥TWS / Only test import and init, don't connect TWS
            from backend.execution_engine import ExecutionEngine
            
            logger.info("  æ­£åœ¨åˆå§‹åŒ–æ‰§è¡Œå¼•æ“ (ä¸è¿æ¥TWS)...")
            
            # æ£€æŸ¥å…³é”®æ–¹æ³•å­˜åœ¨ / Check key methods exist
            engine = ExecutionEngine.__new__(ExecutionEngine)
            
            assert hasattr(ExecutionEngine, '_filter_ai_candidates'), "ç¼ºå°‘_filter_ai_candidatesæ–¹æ³•"
            assert hasattr(ExecutionEngine, '_extract_features_from_analysis'), "ç¼ºå°‘_extract_features_from_analysisæ–¹æ³•"
            assert hasattr(ExecutionEngine, 'run_daily_execution'), "ç¼ºå°‘run_daily_executionæ–¹æ³•"
            
            logger.info("  âœ… æ‰§è¡Œå¼•æ“ç»“æ„éªŒè¯é€šè¿‡")
            logger.info("    - _filter_ai_candidates() âœ“")
            logger.info("    - _extract_features_from_analysis() âœ“")
            logger.info("    - run_daily_execution() âœ“")
            
            self.results['execution_engine'] = True
            return True
            
        except Exception as e:
            logger.error(f"  âŒ æ‰§è¡Œå¼•æ“æµ‹è¯•å¤±è´¥: {e}")
            self.results['execution_engine'] = False
            return False
    
    def test_training_data_exists(self) -> bool:
        """æ£€æŸ¥è®­ç»ƒæ•°æ®æ˜¯å¦å­˜åœ¨ / Check if training data exists"""
        logger.info("\n" + "=" * 50)
        logger.info("ğŸ“ æµ‹è¯•5: è®­ç»ƒæ•°æ®æ£€æŸ¥ / Test 5: Training Data Check")
        logger.info("=" * 50)
        
        data_path = Path(__file__).parent / 'training_data_deep.parquet'
        
        if data_path.exists():
            df = pd.read_parquet(data_path)
            logger.info(f"  âœ… è®­ç»ƒæ•°æ®å­˜åœ¨: {data_path.name}")
            logger.info(f"    - æ ·æœ¬æ•°: {len(df)}")
            logger.info(f"    - åˆ—æ•°: {len(df.columns)}")
            self.results['training_data'] = True
            return True
        else:
            logger.warning(f"  âš ï¸ è®­ç»ƒæ•°æ®ä¸å­˜åœ¨")
            logger.info(f"    â†’ è¯·è¿è¡Œ: python ml/generate_simulated_data.py")
            self.results['training_data'] = 'not_exists'
            return True  # ä¸é˜»æ­¢æµ‹è¯•é€šè¿‡ / Don't block test
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯• / Run all tests"""
        logger.info("=" * 60)
        logger.info("ğŸ§ª ButterQuant ç«¯åˆ°ç«¯æµ‹è¯• / End-to-End Tests")
        logger.info("=" * 60)
        logger.info(f"æ¨¡å¼ / Mode: {'å¿«é€Ÿ' if self.quick_mode else 'å®Œæ•´'}")
        
        tests = [
            ('ç‰¹å¾æå–', self.test_feature_extractor),
            ('MLæ¨ç†', self.test_ml_inference),
            ('æ•°æ®éªŒè¯', self.test_data_validation),
            ('æ‰§è¡Œå¼•æ“', self.test_execution_engine_integration),
            ('è®­ç»ƒæ•°æ®', self.test_training_data_exists),
        ]
        
        passed = 0
        failed = 0
        skipped = 0
        
        for name, test_func in tests:
            try:
                result = test_func()
                if result:
                    passed += 1
                else:
                    failed += 1
            except Exception as e:
                logger.error(f"æµ‹è¯• '{name}' å¼‚å¸¸: {e}")
                failed += 1
        
        # ç»Ÿè®¡è·³è¿‡çš„æµ‹è¯• / Count skipped tests
        for k, v in self.results.items():
            if v == 'skipped' or v == 'not_exists':
                skipped += 1
                passed -= 1  # ä¸ç®—é€šè¿‡ / Don't count as passed
        
        # æ±‡æ€»æŠ¥å‘Š / Summary
        elapsed = time.time() - self.start_time
        
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“‹ æµ‹è¯•æŠ¥å‘Šæ±‡æ€» / Test Summary")
        logger.info("=" * 60)
        logger.info(f"  é€šè¿‡ / Passed:  {passed}")
        logger.info(f"  å¤±è´¥ / Failed:  {failed}")
        logger.info(f"  è·³è¿‡ / Skipped: {skipped}")
        logger.info(f"  è€—æ—¶ / Time:    {elapsed:.1f}s")
        
        if failed == 0:
            logger.info("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡! / All tests passed!")
            return True
        else:
            logger.error(f"\nâŒ {failed} ä¸ªæµ‹è¯•å¤±è´¥! / {failed} tests failed!")
            return False


def main():
    parser = argparse.ArgumentParser(description='ButterQuant ç«¯åˆ°ç«¯æµ‹è¯•')
    parser.add_argument('--quick', action='store_true', help='å¿«é€Ÿæµ‹è¯•æ¨¡å¼ (è·³è¿‡æ€§èƒ½æµ‹è¯•)')
    args = parser.parse_args()
    
    tester = EndToEndTest(quick_mode=args.quick)
    success = tester.run_all_tests()
    
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
