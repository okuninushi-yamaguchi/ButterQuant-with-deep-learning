# ml/rl_agent.py
"""
强化学习权重优化器
目标：学习最优的 calculate_strategy_score 权重分配
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from collections import deque
import random

class WeightOptimizer(nn.Module):
    """
    学习 4 个因子的最优权重
    输入：市场状态特征
    输出：[w1, w2, w3, w4]（权重向量，和为1）
    """
    
    def __init__(self, state_dim=10, hidden_dim=64):
        super().__init__()
        
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(hidden_dim, 4),  # 4 个权重
            nn.Softmax(dim=-1)  # 确保和为 1
        )
        
        # 初始化为固定权重
        self._init_weights()
    
    def _init_weights(self):
        """初始化为 analyzer.py 的固定权重"""
        # 最后一层偏置设为 [35, 30, 20, 15]
        with torch.no_grad():
            last_layer = list(self.network.children())[-2]
            last_layer.bias.copy_(torch.tensor([35.0, 30.0, 20.0, 15.0]))
    
    def forward(self, state):
        """
        输入：市场状态 [VIX, 趋势强度, 波动率百分位, ...]
        输出：动态权重 [w1, w2, w3, w4]
        """
        return self.network(state) * 100  # 转换为百分比


class TradingEnvironment:
    """
    交易环境模拟器
    与盈透证券 API 对接
    """
    
    def __init__(self, account_type='baseline'):
        self.account_type = account_type  # 'baseline' or 'rl'
        self.portfolio_value = 100000.0
        self.positions = []
        self.trade_history = []
        
    def reset(self):
        """重置环境"""
        self.portfolio_value = 100000.0
        self.positions = []
        return self._get_state()
    
    def _get_state(self):
        """
        获取当前市场状态
        返回：numpy array [10 维特征]
        """
        # 从 yfinance 或盈透获取实时数据
        vix = self._get_vix()  # VIX 指数
        spy_trend = self._get_spy_trend()  # SPY 趋势强度
        iv_percentile = self._get_iv_percentile()  # IV 百分位
        
        state = np.array([
            vix,
            spy_trend,
            iv_percentile,
            len(self.positions),  # 当前持仓数
            self.portfolio_value / 100000.0,  # 账户价值（归一化）
            # ... 其他特征
        ], dtype=np.float32)
        
        return state
    
    def step(self, action, ticker):
        """
        执行交易动作
        
        参数:
            action: dict {
                'weights': [w1, w2, w3, w4],  # 动态权重
                'trade': bool  # 是否交易
            }
            ticker: str  # 股票代码
        
        返回:
            next_state, reward, done, info
        """
        from analyzer import ButterflyAnalyzer
        
        # 1. 使用动态权重计算分数
        analyzer = ButterflyAnalyzer(ticker)
        result = analyzer.full_analysis(weights=action['weights'])
        
        # 2. 判断是否交易
        if action['trade'] and result['total_score'] > 70:
            # 执行期权交易（通过盈透 API）
            trade_result = self._execute_butterfly_trade(result)
            self.positions.append(trade_result)
        
        # 3. 更新持仓（每日检查到期、平仓等）
        self._update_positions()
        
        # 4. 计算奖励
        reward = self._calculate_reward()
        
        # 5. 检查是否结束（例如：1个月后）
        done = len(self.trade_history) > 30
        
        next_state = self._get_state()
        info = {
            'portfolio_value': self.portfolio_value,
            'num_positions': len(self.positions)
        }
        
        return next_state, reward, done, info
    
    def _calculate_reward(self):
        """
        计算奖励函数
        核心：RL 账户 vs Baseline 账户的相对表现
        """
        # 计算当日 P&L
        daily_pnl = self._get_daily_pnl()
        
        # 计算 Sharpe Ratio（年化）
        sharpe = self._calculate_sharpe()
        
        # 计算最大回撤
        max_drawdown = self._calculate_max_drawdown()
        
        # 组合奖励
        reward = (
            daily_pnl / 1000.0 +  # P&L 归一化
            sharpe * 0.1 -  # Sharpe 奖励
            max_drawdown * 0.5  # 惩罚大回撤
        )
        
        return reward
    
    def _execute_butterfly_trade(self, analysis_result):
        """
        执行蝴蝶期权交易（连接盈透 API）
        """
        from ib_insync import IB, Option, MarketOrder
        
        ib = IB()
        ib.connect('127.0.0.1', 7497, clientId=1)  # TWS 端口
        
        strategy = analysis_result['butterfly_strategy']
        
        # 构建期权合约
        lower_call = Option(
            symbol=analysis_result['ticker'],
            lastTradeDateOrContractMonth=strategy['expiry'],
            strike=strategy['lower_strike'],
            right='C'
        )
        
        center_call = Option(
            symbol=analysis_result['ticker'],
            lastTradeDateOrContractMonth=strategy['expiry'],
            strike=strategy['center_strike'],
            right='C'
        )
        
        upper_call = Option(
            symbol=analysis_result['ticker'],
            lastTradeDateOrContractMonth=strategy['expiry'],
            strike=strategy['upper_strike'],
            right='C'
        )
        
        # 下单（蝴蝶价差）
        orders = [
            ib.placeOrder(lower_call, MarketOrder('BUY', 1)),
            ib.placeOrder(center_call, MarketOrder('SELL', 2)),
            ib.placeOrder(upper_call, MarketOrder('BUY', 1))
        ]
        
        ib.sleep(1)
        ib.disconnect()
        
        return {
            'ticker': analysis_result['ticker'],
            'entry_date': datetime.now(),
            'cost': strategy['net_cost'],
            'max_profit': strategy['max_profit']
        }
    
    def _get_vix(self):
        """获取 VIX 指数"""
        import yfinance as yf
        vix = yf.Ticker('^VIX')
        return vix.history(period='1d')['Close'].iloc[-1]
    
    def _get_spy_trend(self):
        """计算 SPY 趋势强度"""
        import yfinance as yf
        spy = yf.Ticker('SPY')
        hist = spy.history(period='1mo')
        return (hist['Close'].iloc[-1] / hist['Close'].iloc[0] - 1) * 100
    
    def _get_iv_percentile(self):
        """计算整体市场 IV 百分位"""
        # 简化：用 VIX 的历史百分位
        import yfinance as yf
        vix = yf.Ticker('^VIX')
        hist = vix.history(period='1y')
        current = hist['Close'].iloc[-1]
        percentile = (hist['Close'] < current).sum() / len(hist) * 100
        return percentile
    
    def _get_daily_pnl(self):
        """计算当日 P&L"""
        # 遍历所有持仓，计算未实现盈亏
        total_pnl = 0
        for pos in self.positions:
            # 从盈透获取当前市值
            current_value = self._get_position_value(pos)
            total_pnl += current_value - pos['cost']
        return total_pnl
    
    def _calculate_sharpe(self):
        """计算 Sharpe Ratio"""
        if len(self.trade_history) < 2:
            return 0
        
        returns = np.diff([t['portfolio_value'] for t in self.trade_history])
        returns = returns / self.trade_history[0]['portfolio_value']
        
        sharpe = np.mean(returns) / (np.std(returns) + 1e-6) * np.sqrt(252)
        return sharpe
    
    def _calculate_max_drawdown(self):
        """计算最大回撤"""
        if not self.trade_history:
            return 0
        
        values = [t['portfolio_value'] for t in self.trade_history]
        peak = np.maximum.accumulate(values)
        drawdown = (peak - values) / peak
        return np.max(drawdown)
    
    def _update_positions(self):
        """更新持仓（检查到期、止损等）"""
        for pos in self.positions[:]:
            # 检查是否到期
            if self._is_expired(pos):
                self._close_position(pos)
                self.positions.remove(pos)
            
            # 检查止损（例如亏损 > 50%）
            current_value = self._get_position_value(pos)
            if current_value < pos['cost'] * 0.5:
                self._close_position(pos)
                self.positions.remove(pos)
    
    def _get_position_value(self, position):
        """获取持仓当前市值（从盈透 API）"""
        # 简化实现，实际应从盈透获取
        return position['cost'] + np.random.randn() * 100
    
    def _is_expired(self, position):
        """检查是否到期"""
        from datetime import datetime, timedelta
        expiry = datetime.strptime(position['expiry'], '%Y%m%d')
        return datetime.now() > expiry
    
    def _close_position(self, position):
        """平仓"""
        pnl = self._get_position_value(position) - position['cost']
        self.portfolio_value += pnl
        self.trade_history.append({
            'close_date': datetime.now(),
            'pnl': pnl,
            'portfolio_value': self.portfolio_value
        })


class DQNAgent:
    """
    深度 Q 网络智能体
    用于学习最优权重策略
    """
    
    def __init__(self, state_dim=10, lr=1e-4):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # 主网络
        self.policy_net = WeightOptimizer(state_dim).to(self.device)
        
        # 目标网络（稳定训练）
        self.target_net = WeightOptimizer(state_dim).to(self.device)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)
        
        # 经验回放缓冲区
        self.memory = deque(maxlen=10000)
        
        # 超参数
        self.gamma = 0.99  # 折扣因子
        self.epsilon = 1.0  # 探索率
        self.epsilon_min = 0.05
        self.epsilon_decay = 0.995
        self.batch_size = 64
    
    def select_action(self, state, baseline_weights=[35, 30, 20, 15]):
        """
        选择动作（epsilon-greedy）
        
        返回:
            {
                'weights': [w1, w2, w3, w4],
                'trade': bool
            }
        """
        # ε-greedy 探索
        if random.random() < self.epsilon:
            # 探索：在 baseline 附近随机采样
            weights = np.array(baseline_weights) + np.random.randn(4) * 5
            weights = np.clip(weights, 5, 50)  # 限制范围
            weights = weights / weights.sum() * 100
        else:
            # 利用：使用神经网络预测
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
                weights = self.policy_net(state_tensor).cpu().numpy()[0]
        
        return {
            'weights': weights.tolist(),
            'trade': True  # 简化：总是交易（可以改为另一个网络决定）
        }
    
    def store_transition(self, state, action, reward, next_state, done):
        """存储经验"""
        self.memory.append((state, action, reward, next_state, done))
    
    def learn(self):
        """从经验回放中学习"""
        if len(self.memory) < self.batch_size:
            return
        
        # 采样批次
        batch = random.sample(self.memory, self.batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        
        states = torch.FloatTensor(states).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        next_states = torch.FloatTensor(next_states).to(self.device)
        dones = torch.FloatTensor(dones).to(self.device)
        
        # 计算当前 Q 值
        current_weights = self.policy_net(states)
        
        # 计算目标 Q 值
        with torch.no_grad():
            next_weights = self.target_net(next_states)
            # 这里需要定义 Q 值（简化：用权重与奖励的关系）
            # 实际应该用 Q-learning 公式
            target_q = rewards.unsqueeze(1)
        
        # 计算损失
        loss = nn.MSELoss()(current_weights.sum(dim=1, keepdim=True), target_q)
        
        # 反向传播
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)
        self.optimizer.step()
        
        # 衰减探索率
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)
        
        return loss.item()
    
    def update_target_network(self):
        """更新目标网络"""
        self.target_net.load_state_dict(self.policy_net.state_dict())


# ==================== 训练主循环 ====================
def train_dual_account():
    """
    训练双账户系统
    """
    # 初始化环境
    baseline_env = TradingEnvironment('baseline')
    rl_env = TradingEnvironment('rl')
    
    # 初始化智能体
    agent = DQNAgent(state_dim=10)
    
    # 训练超参数
    num_episodes = 100  # 训练 100 个交易周期（例如每周期 = 1个月）
    update_freq = 10  # 每 10 步更新目标网络
    
    for episode in range(num_episodes):
        # 重置环境
        baseline_state = baseline_env.reset()
        rl_state = rl_env.reset()
        
        episode_reward = 0
        
        # 获取当周要交易的股票列表
        tickers = get_weekly_scan_results()  # 从 daily_scanner.py 获取
        
        for day in range(30):  # 每个周期 30 天
            for ticker in tickers:
                # === Baseline 账户（固定权重）===
                baseline_action = {
                    'weights': [35, 30, 20, 15],
                    'trade': True
                }
                baseline_env.step(baseline_action, ticker)
                
                # === RL 账户（动态权重）===
                rl_action = agent.select_action(rl_state)
                next_state, reward, done, info = rl_env.step(rl_action, ticker)
                
                # 计算相对奖励（RL vs Baseline）
                relative_reward = (
                    info['portfolio_value'] - baseline_env.portfolio_value
                ) / 1000.0
                
                # 存储经验
                agent.store_transition(rl_state, rl_action, relative_reward, next_state, done)
                
                # 学习
                loss = agent.learn()
                
                # 更新状态
                rl_state = next_state
                episode_reward += relative_reward
                
                if done:
                    break
            
            # 定期更新目标网络
            if day % update_freq == 0:
                agent.update_target_network()
        
        # 记录结果
        print(f"Episode {episode + 1}/{num_episodes}")
        print(f"  Baseline P&L: ${baseline_env.portfolio_value - 100000:.2f}")
        print(f"  RL P&L: ${rl_env.portfolio_value - 100000:.2f}")
        print(f"  Relative Reward: {episode_reward:.2f}")
        print(f"  Epsilon: {agent.epsilon:.4f}")
        
        # 保存模型
        if (episode + 1) % 10 == 0:
            torch.save(agent.policy_net.state_dict(), f'models/rl_agent_ep{episode+1}.pth')


def get_weekly_scan_results():
    """获取每周扫描结果（高分股票）"""
    from database import db_manager
    results = db_manager.get_top_opportunities(limit=20, min_score=70)
    return [r['ticker'] for r in results]


if __name__ == '__main__':
    train_dual_account()