# scripts/export_for_ml.py
"""
ä» SQLite å¯¼å‡ºè®­ç»ƒæ•°æ®åˆ° Parquet æ ¼å¼
ç”¨æ³•ï¼špython export_for_ml.py --output-dir ./ml_data
"""

import sqlite3
import pandas as pd
import argparse
from pathlib import Path
from datetime import datetime
import json

def export_training_data(
    db_path='backend/data/market_research.db',
    output_dir='ml_data',
    min_score=50,
    lookback_days=180
):
    """
    å¯¼å‡ºæ·±åº¦å­¦ä¹ è®­ç»ƒæ•°æ®
    
    å‚æ•°:
        db_path: SQLite æ•°æ®åº“è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        min_score: æœ€ä½åˆ†æ•°ï¼ˆè¿‡æ»¤ä½è´¨é‡æ•°æ®ï¼‰
        lookback_days: å›æº¯å¤©æ•°
    """
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # è¿æ¥æ•°æ®åº“
    conn = sqlite3.connect(db_path)
    
    # ==================== æŸ¥è¯¢æ•°æ® ====================
    query = f"""
    SELECT 
        ticker,
        analysis_date,
        current_price,
        
        -- å‚…ç«‹å¶ç‰¹å¾
        trend_direction,
        trend_slope,
        dominant_period,
        
        -- ARIMA é¢„æµ‹
        predicted_price,
        prediction_lower,
        prediction_upper,
        price_stability,
        
        -- GARCH æ³¢åŠ¨ç‡
        predicted_vol,
        current_iv,
        vol_mispricing,
        iv_percentile,
        
        -- Greeks
        delta,
        gamma,
        vega,
        theta,
        
        -- ç­–ç•¥æŒ‡æ ‡
        butterfly_type,
        max_profit,
        max_loss,
        profit_ratio,
        prob_profit,
        
        -- è¯„åˆ†
        total_score,
        recommendation
        
    FROM daily_metrics
    WHERE total_score >= {min_score}
      AND analysis_date >= DATE('now', '-{lookback_days} days')
    ORDER BY analysis_date, ticker
    """
    
    print(f"ğŸ“Š æ­£åœ¨ä» SQLite è¯»å–æ•°æ®...")
    df = pd.read_sql_query(query, conn)
    conn.close()
    
    print(f"âœ… è¯»å–å®Œæˆï¼š{len(df)} æ¡è®°å½•")
    print(f"   æ—¶é—´èŒƒå›´ï¼š{df['analysis_date'].min()} ~ {df['analysis_date'].max()}")
    print(f"   è‚¡ç¥¨æ•°é‡ï¼š{df['ticker'].nunique()}")
    
    # ==================== æ•°æ®æ¸…æ´— ====================
    print(f"\nğŸ§¹ æ•°æ®æ¸…æ´—...")
    
    # 1. åˆ é™¤ç¼ºå¤±å€¼
    before = len(df)
    df = df.dropna()
    print(f"   åˆ é™¤ç¼ºå¤±å€¼ï¼š{before - len(df)} æ¡")
    
    # 2. åˆ é™¤å¼‚å¸¸å€¼ï¼ˆä»·æ ¼ < 0 ç­‰ï¼‰
    df = df[df['current_price'] > 0]
    df = df[df['predicted_vol'] > 0]
    df = df[df['current_iv'] > 0]
    
    # 3. è½¬æ¢åˆ†ç±»å˜é‡
    df['trend_direction'] = df['trend_direction'].map({
        'UPTREND': 1,
        'DOWNTREND': -1,
        'SIDEWAYS': 0
    })
    
    df['butterfly_type'] = df['butterfly_type'].map({
        'CALL': 1,
        'PUT': 2,
        'IRON': 3
    })
    
    df['recommendation'] = df['recommendation'].map({
        'STRONG_BUY': 4,
        'BUY': 3,
        'NEUTRAL': 2,
        'AVOID': 1
    })
    
    # ==================== ç‰¹å¾å·¥ç¨‹ ====================
    print(f"\nğŸ”§ ç‰¹å¾å·¥ç¨‹...")
    
    # 1. æ—¶åºç‰¹å¾ï¼ˆæŒ‰è‚¡ç¥¨åˆ†ç»„ï¼‰
    df['analysis_date'] = pd.to_datetime(df['analysis_date'])
    df = df.sort_values(['ticker', 'analysis_date'])
    
    # æ»šåŠ¨ç»Ÿè®¡ï¼ˆ7å¤©çª—å£ï¼‰
    df['price_ma7'] = df.groupby('ticker')['current_price'].transform(
        lambda x: x.rolling(7, min_periods=1).mean()
    )
    df['price_std7'] = df.groupby('ticker')['current_price'].transform(
        lambda x: x.rolling(7, min_periods=1).std()
    )
    df['vol_ma7'] = df.groupby('ticker')['predicted_vol'].transform(
        lambda x: x.rolling(7, min_periods=1).mean()
    )
    
    # 2. ç›¸å¯¹ç‰¹å¾
    df['price_vs_ma7'] = df['current_price'] / df['price_ma7']
    df['vol_vs_ma7'] = df['predicted_vol'] / df['vol_ma7']
    
    # 3. æ ‡ç­¾ï¼šæœªæ¥æ”¶ç›Šç‡ï¼ˆ7å¤©åï¼‰
    df['future_price_7d'] = df.groupby('ticker')['current_price'].shift(-7)
    df['future_return_7d'] = (df['future_price_7d'] / df['current_price'] - 1) * 100
    
    # äºŒåˆ†ç±»æ ‡ç­¾ï¼šæ˜¯å¦æ¨ªç›˜ï¼ˆ|æ”¶ç›Šç‡| < 2%ï¼‰
    df['label_sideways'] = (df['future_return_7d'].abs() < 2).astype(int)
    
    # ä¸‰åˆ†ç±»æ ‡ç­¾ï¼šä¸Šæ¶¨/æ¨ªç›˜/ä¸‹è·Œ
    df['label_direction'] = pd.cut(
        df['future_return_7d'],
        bins=[-float('inf'), -2, 2, float('inf')],
        labels=[0, 1, 2]  # ä¸‹è·Œ/æ¨ªç›˜/ä¸Šæ¶¨
    ).astype(int)
    
    # åˆ é™¤æ²¡æœ‰æœªæ¥æ•°æ®çš„è¡Œï¼ˆæœ€å7å¤©ï¼‰
    df = df[df['future_return_7d'].notna()]
    
    print(f"   æ·»åŠ ç‰¹å¾ï¼šprice_ma7, price_std7, vol_ma7, future_return_7d")
    print(f"   æœ€ç»ˆæ ·æœ¬æ•°ï¼š{len(df)}")
    
    # ==================== è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†åˆ’åˆ† ====================
    print(f"\nğŸ“¦ åˆ’åˆ†æ•°æ®é›†...")
    
    # æŒ‰æ—¶é—´åˆ’åˆ†ï¼ˆé¿å…æ•°æ®æ³„éœ²ï¼‰
    df = df.sort_values('analysis_date')
    train_size = int(len(df) * 0.7)
    val_size = int(len(df) * 0.15)
    
    train_df = df.iloc[:train_size]
    val_df = df.iloc[train_size:train_size + val_size]
    test_df = df.iloc[train_size + val_size:]
    
    print(f"   è®­ç»ƒé›†ï¼š{len(train_df)} ({train_df['analysis_date'].min()} ~ {train_df['analysis_date'].max()})")
    print(f"   éªŒè¯é›†ï¼š{len(val_df)} ({val_df['analysis_date'].min()} ~ {val_df['analysis_date'].max()})")
    print(f"   æµ‹è¯•é›†ï¼š{len(test_df)} ({test_df['analysis_date'].min()} ~ {test_df['analysis_date'].max()})")
    
    # ==================== ä¿å­˜ Parquet ====================
    print(f"\nğŸ’¾ ä¿å­˜æ•°æ®...")
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    train_path = output_path / f'train_{timestamp}.parquet'
    val_path = output_path / f'val_{timestamp}.parquet'
    test_path = output_path / f'test_{timestamp}.parquet'
    
    train_df.to_parquet(train_path, compression='snappy', index=False)
    val_df.to_parquet(val_path, compression='snappy', index=False)
    test_df.to_parquet(test_path, compression='snappy', index=False)
    
    print(f"   âœ… è®­ç»ƒé›†ï¼š{train_path} ({train_path.stat().st_size / 1024 / 1024:.2f} MB)")
    print(f"   âœ… éªŒè¯é›†ï¼š{val_path} ({val_path.stat().st_size / 1024 / 1024:.2f} MB)")
    print(f"   âœ… æµ‹è¯•é›†ï¼š{test_path} ({test_path.stat().st_size / 1024 / 1024:.2f} MB)")
    
    # ==================== ä¿å­˜å…ƒæ•°æ® ====================
    metadata = {
        'export_date': timestamp,
        'db_path': db_path,
        'total_samples': len(df),
        'train_samples': len(train_df),
        'val_samples': len(val_df),
        'test_samples': len(test_df),
        'features': list(df.columns),
        'label_distribution': {
            'sideways': df['label_sideways'].value_counts().to_dict(),
            'direction': df['label_direction'].value_counts().to_dict()
        }
    }
    
    metadata_path = output_path / f'metadata_{timestamp}.json'
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“„ å…ƒæ•°æ®ï¼š{metadata_path}")
    
    return train_df, val_df, test_df

# ==================== å‘½ä»¤è¡Œå…¥å£ ====================
if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='å¯¼å‡ºæ·±åº¦å­¦ä¹ è®­ç»ƒæ•°æ®')
    parser.add_argument('--db', default='backend/data/market_research.db', help='æ•°æ®åº“è·¯å¾„')
    parser.add_argument('--output-dir', default='ml_data', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--min-score', type=int, default=50, help='æœ€ä½åˆ†æ•°')
    parser.add_argument('--lookback-days', type=int, default=180, help='å›æº¯å¤©æ•°')
    
    args = parser.parse_args()
    
    export_training_data(
        db_path=args.db,
        output_dir=args.output_dir,
        min_score=args.min_score,
        lookback_days=args.lookback_days
    )
    
    print("\nâœ… å¯¼å‡ºå®Œæˆï¼")
    print("\nğŸ“ ä¸‹ä¸€æ­¥ï¼š")
    print("   1. cd ml_data")
    print("   2. python train.py --data train_*.parquet")