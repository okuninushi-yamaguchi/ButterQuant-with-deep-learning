# ml/bayesian_optimizer.py
"""
贝叶斯权重优化器
使用概率分布建模权重，而非固定值
"""

import torch
import torch.nn as nn
import torch.distributions as dist
import numpy as np

class BayesianWeightOptimizer(nn.Module):
    """
    贝叶斯神经网络：学习权重的分布
    
    核心思想：
    权重不是固定的 [35%, 30%, 20%, 15%]
    而是服从某个分布，例如：
    w1 ~ Beta(α1, β1)  # 价格匹配度权重
    w2 ~ Beta(α2, β2)  # 波动率权重
    w3 ~ Beta(α3, β3)  # 稳定性权重
    w4 ~ Beta(α4, β4)  # 傅立叶权重
    """
    
    def __init__(self, state_dim=10):
        super().__init__()
        
        # 状态编码器
        self.encoder = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU()
        )
        
        # 为每个权重学习 Beta 分布的参数
        # Beta 分布：w ~ Beta(α, β)，范围 [0, 1]
        self.alpha_head = nn.Linear(64, 4)  # 输出 [α1, α2, α3, α4]
        self.beta_head = nn.Linear(64, 4)   # 输出 [β1, β2, β3, β4]
        
        # 初始化为固定权重对应的分布
        self._init_to_baseline()
    
    def _init_to_baseline(self):
        """
        初始化为 baseline 权重 [35, 30, 20, 15]
        
        Beta 分布的均值：E[X] = α / (α + β)
        要让 E[X] = 0.35，可以设 α=3.5, β=6.5（α+β=10）
        """
        baseline = torch.tensor([35, 30, 20, 15]) / 100.0  # 归一化
        
        # 使用矩估计初始化 α, β
        # 假设方差较小（高置信度），设 α+β = 20
        precision = 20.0
        with torch.no_grad():
            self.alpha_head.bias.copy_(baseline * precision)
            self.beta_head.bias.copy_((1 - baseline) * precision)
    
    def forward(self, state, sample=True):
        """
        前向传播
        
        参数:
            state: 市场状态 [batch, state_dim]
            sample: 是否采样（训练时 True，推理时 False）
        
        返回:
            weights: [batch, 4]，权重向量（和为1）
            log_prob: 对数概率（用于策略梯度）
        """
        # 编码状态
        features = self.encoder(state)
        
        # 预测 Beta 分布参数
        alpha = torch.softplus(self.alpha_head(features)) + 1e-3  # 确保 > 0
        beta = torch.softplus(self.beta_head(features)) + 1e-3
        
        # 创建 Beta 分布
        beta_dist = dist.Beta(alpha, beta)
        
        if sample:
            # 训练时：采样
            weights_raw = beta_dist.rsample()  # 可微分采样
            log_prob = beta_dist.log_prob(weights_raw).sum(dim=-1)
        else:
            # 推理时：使用均值
            weights_raw = beta_dist.mean
            log_prob = None
        
        # 归一化（确保和为1）
        weights = weights_raw / weights_raw.sum(dim=-1, keepdim=True) * 100
        
        return weights, log_prob, (alpha, beta)
    
    def get_uncertainty(self, state):
        """
        计算权重的不确定性（熵）
        
        返回:
            entropy: [batch, 4]，每个权重的熵
        """
        features = self.encoder(state)
        alpha = torch.softplus(self.alpha_head(features)) + 1e-3
        beta = torch.softplus(self.beta_head(features)) + 1e-3
        
        beta_dist = dist.Beta(alpha, beta)
        entropy = beta_dist.entropy()  # 熵越大，不确定性越高
        
        return entropy


class PolicyGradientAgent:
    """
    策略梯度智能体（用于贝叶斯优化）
    """
    
    def __init__(self, state_dim=10, lr=1e-4):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = BayesianWeightOptimizer(state_dim).to(self.device)
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)
        
        # 存储轨迹
        self.trajectories = []
    
    def select_action(self, state):
        """
        选择动作（采样权重）
        """
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            weights, _, (alpha, beta) = self.model(state_tensor, sample=True)
        
        return {
            'weights': weights[0].cpu().numpy(),
            'alpha': alpha[0].cpu().numpy(),
            'beta': beta[0].cpu().numpy()
        }
    
    def store_trajectory(self, state, action, reward):
        """存储轨迹"""
        self.trajectories.append({
            'state': state,
            'weights': action['weights'],
            'reward': reward
        })
    
    def learn(self):
        """
        策略梯度更新
        使用 REINFORCE 算法
        """
        if not self.trajectories:
            return
        
        # 计算折扣回报
        rewards = [t['reward'] for t in self.trajectories]
        returns = self._compute_returns(rewards, gamma=0.99)
        
        # 归一化回报（减少方差）
        returns = (returns - returns.mean()) / (returns.std() + 1e-8)
        
        # 计算策略梯度损失
        total_loss = 0
        for traj, G in zip(self.trajectories, returns):
            state = torch.FloatTensor(traj['state']).unsqueeze(0).to(self.device)
            
            # 重新计算 log_prob
            weights, log_prob, _ = self.model(state, sample=False)
            
            # 策略梯度：-log_prob * G
            loss = -log_prob * G
            total_loss += loss
        
        # 反向传播
        self.optimizer.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
        self.optimizer.step()
        
        # 清空轨迹
        self.trajectories = []
        
        return total_loss.item()
    
    def _compute_returns(self, rewards, gamma=0.99):
        """计算折扣回报"""
        returns = []
        G = 0
        for r in reversed(rewards):
            G = r + gamma * G
            returns.insert(0, G)
        return torch.FloatTensor(returns)
    
    def get_weight_distribution(self, state):
        """
        获取权重分布（用于可视化）
        
        返回:
            {
                'mean': [w1, w2, w3, w4],
                'std': [σ1, σ2, σ3, σ4],
                'ci_lower': 95% 置信区间下界,
                'ci_upper': 95% 置信区间上界
            }
        """
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            features = self.model.encoder(state_tensor)
            alpha = torch.softplus(self.model.alpha_head(features)) + 1e-3
            beta = torch.softplus(self.model.beta_head(features)) + 1e-3
            
            # Beta 分布统计量
            mean = alpha / (alpha + beta)
            var = (alpha * beta) / ((alpha + beta)**2 * (alpha + beta + 1))
            std = torch.sqrt(var)
            
            # 95% 置信区间（近似）
            ci_lower = torch.clamp(mean - 1.96 * std, 0, 1)
            ci_upper = torch.clamp(mean + 1.96 * std, 0, 1)
        
        return {
            'mean': (mean[0] * 100).cpu().numpy(),
            'std': (std[0] * 100).cpu().numpy(),
            'ci_lower': (ci_lower[0] * 100).cpu().numpy(),
            'ci_upper': (ci_upper[0] * 100).cpu().numpy()
        }


# ==================== 训练示例 ====================
def train_bayesian_agent():
    """
    训练贝叶斯智能体
    """
    from ml.rl_agent import TradingEnvironment
    
    env = TradingEnvironment('rl')
    agent = PolicyGradientAgent(state_dim=10, lr=1e-4)
    
    num_episodes = 100
    
    for episode in range(num_episodes):
        state = env.reset()
        episode_reward = 0
        
        for day in range(30):
            # 选择动作
            action = agent.select_action(state)
            
            # 执行动作
            next_state, reward, done, info = env.step(action, ticker='AAPL')
            
            # 存储轨迹
            agent.store_trajectory(state, action, reward)
            
            state = next_state
            episode_reward += reward
            
            if done:
                break
        
        # 周期结束，学习
        loss = agent.learn()
        
        # 打印权重分布
        dist_info = agent.get_weight_distribution(state)
        print(f"\nEpisode {episode + 1}/{num_episodes}")
        print(f"  Total Reward: {episode_reward:.2f}")
        print(f"  Loss: {loss:.4f}")
        print(f"  Weight Distribution:")
        print(f"    价格匹配: {dist_info['mean'][0]:.1f}% ± {dist_info['std'][0]:.1f}%")
        print(f"    波动率:   {dist_info['mean'][1]:.1f}% ± {dist_info['std'][1]:.1f}%")
        print(f"    稳定性:   {dist_info['mean'][2]:.1f}% ± {dist_info['std'][2]:.1f}%")
        print(f"    傅立叶:   {dist_info['mean'][3]:.1f}% ± {dist_info['std'][3]:.1f}%")


if __name__ == '__main__':
    train_bayesian_agent()