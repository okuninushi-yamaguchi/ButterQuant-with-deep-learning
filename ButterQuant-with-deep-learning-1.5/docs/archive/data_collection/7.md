"""
股票代码工具
用于处理Nasdaq 100和S&P 500的股票列表
"""

from typing import List, Set
import requests
from pathlib import Path


class TickerUtils:
    """股票代码工具类"""
    
    @staticmethod
    def fetch_nasdaq100() -> List[str]:
        """
        从在线源获取Nasdaq 100成分股
        可以使用Wikipedia或其他数据源
        """
        try:
            # 方法1: 从Wikipedia获取
            url = "https://en.wikipedia.org/wiki/Nasdaq-100"
            import pandas as pd
            tables = pd.read_html(url)
            
            # 通常第一个或第二个表格包含成分股
            for table in tables:
                if 'Ticker' in table.columns or 'Symbol' in table.columns:
                    ticker_col = 'Ticker' if 'Ticker' in table.columns else 'Symbol'
                    tickers = table[ticker_col].tolist()
                    # 清理数据
                    tickers = [str(t).strip() for t in tickers if pd.notna(t)]
                    return tickers
            
            return []
        except Exception as e:
            print(f"Failed to fetch Nasdaq 100: {e}")
            return []
    
    @staticmethod
    def fetch_sp500() -> List[str]:
        """
        从在线源获取S&P 500成分股
        """
        try:
            # 从Wikipedia获取
            url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
            import pandas as pd
            tables = pd.read_html(url)
            
            # 第一个表格通常是成分股列表
            df = tables[0]
            if 'Symbol' in df.columns:
                tickers = df['Symbol'].tolist()
                # 清理数据（有些股票可能有特殊字符）
                tickers = [str(t).replace('.', '-').strip() for t in tickers if pd.notna(t)]
                return tickers
            
            return []
        except Exception as e:
            print(f"Failed to fetch S&P 500: {e}")
            return []
    
    @staticmethod
    def merge_and_deduplicate(list1: List[str], list2: List[str]) -> List[str]:
        """
        合并两个列表并去重
        """
        combined = set(list1) | set(list2)
        return sorted(list(combined))
    
    @staticmethod
    def save_to_file(tickers: List[str], filename: str):
        """
        保存股票列表到文件
        格式: 每行一个ticker
        """
        filepath = Path(filename)
        filepath.parent.mkdir(parents=True, exist_ok=True)
        
        with open(filepath, 'w') as f:
            for ticker in tickers:
                f.write(f"{ticker}\n")
        
        print(f"Saved {len(tickers)} tickers to {filename}")
    
    @staticmethod
    def load_from_file(filename: str) -> List[str]:
        """
        从文件加载股票列表
        """
        filepath = Path(filename)
        if not filepath.exists():
            return []
        
        with open(filepath, 'r') as f:
            tickers = [line.strip() for line in f if line.strip()]
        
        return tickers
    
    @staticmethod
    def update_ticker_files():
        """
        更新股票列表文件
        从在线源获取最新数据
        """
        print("Fetching Nasdaq 100...")
        nas100 = TickerUtils.fetch_nasdaq100()
        print(f"Found {len(nas100)} Nasdaq 100 stocks")
        
        print("Fetching S&P 500...")
        sp500 = TickerUtils.fetch_sp500()
        print(f"Found {len(sp500)} S&P 500 stocks")
        
        # 保存到文件
        TickerUtils.save_to_file(nas100, "backend/data/nas100.md")
        TickerUtils.save_to_file(sp500, "backend/data/sp500.md")
        
        # 统计重复
        combined = TickerUtils.merge_and_deduplicate(nas100, sp500)
        duplicates = len(nas100) + len(sp500) - len(combined)
        
        print(f"\nTotal unique stocks: {len(combined)}")
        print(f"Duplicates removed: {duplicates}")
        
        # 保存合并后的列表
        TickerUtils.save_to_file(combined, "backend/data/combined_tickers.md")
        
        return combined


# 命令行运行
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == '--update':
        print("Updating ticker files from online sources...")
        TickerUtils.update_ticker_files()
    else:
        print("Usage:")
        print("  python ticker_utils.py --update    Update ticker files from online sources")