# ButterQuant ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æ–¹æ¡ˆ

## ğŸ“Š é—®é¢˜è¯Šæ–­

### å½“å‰æ¶æ„çš„æ€§èƒ½ç“¶é¢ˆ

æ‚¨çš„é¡¾é—®è¯´å¾—å¯¹ã€‚å½“å‰æ¶æ„å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š

1. **Flask å•çº¿ç¨‹é˜»å¡**
   - Flask é»˜è®¤æ˜¯å•è¿›ç¨‹å•çº¿ç¨‹ï¼Œå³ä½¿ç”¨ `gunicorn` ä¹Ÿåªèƒ½å¤„ç†æœ‰é™å¹¶å‘
   - æ‚¨çš„ `analyzer.py` åŒ…å«å¤§é‡è®¡ç®—å¯†é›†å‹æ“ä½œï¼ˆARIMAã€GARCHã€FFTã€Black-Scholesï¼‰
   - å•æ¬¡åˆ†æè€—æ—¶ **5-15ç§’**ï¼Œé«˜å¹¶å‘ä¸‹ä¼šå¯¼è‡´è¯·æ±‚æ’é˜Ÿ

2. **å®æ—¶è®¡ç®—çš„æ€§èƒ½é—®é¢˜**
   ```python
   # app.py ç¬¬135è¡Œ - æ¯æ¬¡è¯·æ±‚éƒ½è§¦å‘å®Œæ•´åˆ†æ
   @app.route('/api/analyze', methods=['POST'])
   def analyze():
       analyzer = ButterflyAnalyzer(ticker)  # å®æ—¶ä¸‹è½½æ•°æ®
       result = analyzer.full_analysis()     # 5-15ç§’è®¡ç®—
   ```

3. **æ•°æ®åº“å¹¶å‘ç“¶é¢ˆ**
   - SQLite åœ¨é«˜å¹¶å‘å†™å…¥æ—¶ä¼šé”è¡¨
   - æ‚¨çš„ `daily_scanner.py` ä½¿ç”¨å¤šçº¿ç¨‹å†™å…¥ï¼Œä½† SQLite ä¸é€‚åˆç”Ÿäº§ç¯å¢ƒ

4. **æ— ç¼“å­˜æœºåˆ¶**
   - ç›¸åŒè‚¡ç¥¨çš„é‡å¤è¯·æ±‚ä¼šé‡å¤è®¡ç®—
   - æ²¡æœ‰ CDN æˆ–è¾¹ç¼˜ç¼“å­˜

---

## ğŸ¯ ç”Ÿäº§çº§è§£å†³æ–¹æ¡ˆï¼ˆ3ä¸ªæ–¹æ¡ˆï¼‰

### æ–¹æ¡ˆ 1ï¼šæœ€å°æ”¹åŠ¨æ–¹æ¡ˆï¼ˆæ¨èç”¨äºå¿«é€Ÿä¸Šçº¿ï¼‰

**æ ¸å¿ƒæ€è·¯**ï¼šä¿æŒ Python åç«¯ï¼Œä½†å°†è®¡ç®—å¼‚æ­¥åŒ– + å¢åŠ ç¼“å­˜

#### æ¶æ„å›¾
```
ç”¨æˆ·è¯·æ±‚ â†’ Nginx â†’ Flask (API Gateway)
                      â†“
                   Redis ç¼“å­˜ (æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜ç»“æœ)
                      â†“ (cache miss)
                   Celery ä»»åŠ¡é˜Ÿåˆ—
                      â†“
                   Celery Workers (å¤šè¿›ç¨‹è®¡ç®—)
                      â†“
                   PostgreSQL (å­˜å‚¨ç»“æœ)
```

#### æŠ€æœ¯æ ˆè°ƒæ•´
| ç»„ä»¶ | å½“å‰ | å‡çº§å |
|------|------|--------|
| WebæœåŠ¡å™¨ | Flask Dev Server | **Nginx + Gunicorn (4-8 workers)** |
| æ•°æ®åº“ | SQLite | **PostgreSQL** (æ”¯æŒé«˜å¹¶å‘) |
| ç¼“å­˜ | æ—  | **Redis** (ç¼“å­˜åˆ†æç»“æœ) |
| ä»»åŠ¡é˜Ÿåˆ— | æ—  | **Celery + Redis** (å¼‚æ­¥è®¡ç®—) |
| éƒ¨ç½² | æœ¬åœ° | **Docker Compose** |

#### æ”¹åŠ¨æ¸…å•

**1. å°†è®¡ç®—å¯†é›†å‹ä»»åŠ¡ç§»åˆ° Celery**
```python
# backend/tasks.py (æ–°å»º)
from celery import Celery
import redis
import json

celery_app = Celery('butterquant', broker='redis://localhost:6379/0')
redis_client = redis.Redis(host='localhost', port=6379, db=1, decode_responses=True)

@celery_app.task
def analyze_ticker_async(ticker):
    """å¼‚æ­¥åˆ†æä»»åŠ¡"""
    from analyzer import ButterflyAnalyzer
    
    # æ£€æŸ¥ç¼“å­˜ (5åˆ†é’Ÿæœ‰æ•ˆæœŸ)
    cache_key = f"analysis:{ticker}"
    cached = redis_client.get(cache_key)
    if cached:
        return json.loads(cached)
    
    # æ‰§è¡Œåˆ†æ
    analyzer = ButterflyAnalyzer(ticker)
    result = analyzer.full_analysis()
    
    # å­˜å…¥ç¼“å­˜
    redis_client.setex(cache_key, 300, json.dumps(result))
    return result
```

**2. API æ”¹ä¸ºå¼‚æ­¥æ¨¡å¼**
```python
# backend/app.py ä¿®æ”¹
from tasks import analyze_ticker_async
import redis

redis_client = redis.Redis(host='localhost', port=6379, db=1, decode_responses=True)

@app.route('/api/analyze', methods=['POST'])
def analyze():
    ticker = request.json.get('ticker', 'AAPL').upper()
    
    # å…ˆæ£€æŸ¥ Redis ç¼“å­˜
    cache_key = f"analysis:{ticker}"
    cached = redis_client.get(cache_key)
    if cached:
        return jsonify({'success': True, 'data': json.loads(cached), 'from_cache': True})
    
    # æäº¤å¼‚æ­¥ä»»åŠ¡
    task = analyze_ticker_async.delay(ticker)
    
    # è¿”å›ä»»åŠ¡IDï¼Œå‰ç«¯è½®è¯¢
    return jsonify({'success': True, 'task_id': task.id, 'status': 'processing'})

@app.route('/api/task/<task_id>', methods=['GET'])
def get_task_status(task_id):
    """æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€"""
    task = analyze_ticker_async.AsyncResult(task_id)
    if task.ready():
        return jsonify({'status': 'completed', 'result': task.result})
    else:
        return jsonify({'status': 'processing'})
```

**3. å‰ç«¯æ”¹ä¸ºè½®è¯¢æ¨¡å¼**
```typescript
// src/components/OptionAnalyzer.tsx
async function analyzeStock(ticker: string) {
  setLoading(true);
  
  const response = await fetch('/api/analyze', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ ticker })
  });
  const data = await response.json();
  
  if (data.from_cache) {
    // ç›´æ¥æ˜¾ç¤ºç¼“å­˜ç»“æœ
    setResult(data.data);
    setLoading(false);
  } else {
    // è½®è¯¢ä»»åŠ¡çŠ¶æ€
    const taskId = data.task_id;
    const interval = setInterval(async () => {
      const statusRes = await fetch(`/api/task/${taskId}`);
      const status = await statusRes.json();
      
      if (status.status === 'completed') {
        clearInterval(interval);
        setResult(status.result);
        setLoading(false);
      }
    }, 1000); // æ¯ç§’æŸ¥è¯¢ä¸€æ¬¡
  }
}
```

#### éƒ¨ç½²é…ç½®

**docker-compose.yml**
```yaml
version: '3.8'
services:
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./dist:/usr/share/nginx/html
    depends_on:
      - flask

  flask:
    build: ./backend
    command: gunicorn -w 4 -b 0.0.0.0:5000 app:app
    environment:
      - DATABASE_URL=postgresql://user:pass@postgres:5432/butterquant
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      - postgres
      - redis

  celery_worker:
    build: ./backend
    command: celery -A tasks worker --loglevel=info --concurrency=4
    environment:
      - DATABASE_URL=postgresql://user:pass@postgres:5432/butterquant
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      - redis
      - postgres

  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: butterquant
      POSTGRES_USER: user
      POSTGRES_PASSWORD: pass
    volumes:
      - postgres_data:/var/lib/postgresql/data

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

volumes:
  postgres_data:
```

**backend/Dockerfile**
```dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt celery redis

COPY . .

CMD ["gunicorn", "-w", "4", "-b", "0.0.0.0:5000", "app:app"]
```

**nginx.conf**
```nginx
events {
    worker_connections 1024;
}

http {
    upstream flask_backend {
        server flask:5000;
    }

    server {
        listen 80;
        
        # å‰ç«¯é™æ€æ–‡ä»¶
        location / {
            root /usr/share/nginx/html;
            try_files $uri $uri/ /index.html;
        }
        
        # API ä»£ç†
        location /api/ {
            proxy_pass http://flask_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_read_timeout 300s;
        }
    }
}
```

#### æˆæœ¬ä¼°ç®—ï¼ˆäº‘æœåŠ¡å•†ï¼‰
- **é˜¿é‡Œäº‘/è…¾è®¯äº‘**ï¼šçº¦ Â¥300-500/æœˆ
  - 2æ ¸4G ECS Ã— 1ï¼ˆè¿è¡Œæ‰€æœ‰å®¹å™¨ï¼‰
  - Redis 1G Ã— 1
  - PostgreSQL 20G Ã— 1
- **é¢„æœŸæ€§èƒ½**ï¼šæ”¯æŒ **100-200 å¹¶å‘ç”¨æˆ·**

---

### ğŸš€ æ–¹æ¡ˆ 2ï¼šæ··åˆæ¶æ„ï¼ˆæ¨èç”¨äºä¸­é•¿æœŸï¼‰

**æ ¸å¿ƒæ€è·¯**ï¼šä¿ç•™ Python è®¡ç®—å¼•æ“ï¼Œä½†ç”¨ **Node.js/Go** åš API å±‚

#### ä¸ºä»€ä¹ˆè¿™æ ·åšï¼Ÿ
- Python æ“…é•¿ç§‘å­¦è®¡ç®—ï¼ˆNumPyã€Pandasï¼‰ï¼Œä½†ä¸æ“…é•¿é«˜å¹¶å‘ I/O
- Node.js/Go æ“…é•¿é«˜å¹¶å‘ API å¤„ç†ï¼Œä½†ç§‘å­¦è®¡ç®—åº“ä¸å¦‚ Python æˆç†Ÿ
- **åˆ†å·¥åˆä½œ**ï¼šNode.js å¤„ç† API + ç¼“å­˜ï¼ŒPython ä¸“æ³¨è®¡ç®—

#### æ¶æ„å›¾
```
ç”¨æˆ· â†’ Cloudflare CDN â†’ Node.js API (Express/Fastify)
                            â†“
                         Redis ç¼“å­˜
                            â†“ (cache miss)
                         RabbitMQ é˜Ÿåˆ—
                            â†“
                         Python Workers (Celery)
                            â†“
                         PostgreSQL
```

#### æŠ€æœ¯æ ˆ
| å±‚çº§ | æŠ€æœ¯ | ä½œç”¨ |
|------|------|------|
| CDN | Cloudflare | é™æ€èµ„æº + è¾¹ç¼˜ç¼“å­˜ |
| API å±‚ | **Node.js (Fastify)** | é«˜å¹¶å‘ API å¤„ç† |
| è®¡ç®—å±‚ | Python (Celery) | é‡åŒ–è®¡ç®— |
| ç¼“å­˜ | Redis Cluster | åˆ†å¸ƒå¼ç¼“å­˜ |
| æ•°æ®åº“ | PostgreSQL + TimescaleDB | æ—¶åºæ•°æ®ä¼˜åŒ– |
| æ¶ˆæ¯é˜Ÿåˆ— | RabbitMQ | ä»»åŠ¡åˆ†å‘ |

#### Node.js API ç¤ºä¾‹
```javascript
// api/server.js
const fastify = require('fastify')();
const redis = require('redis');
const amqp = require('amqplib');
const { v4: uuidv4 } = require('uuid');

const redisClient = redis.createClient();

fastify.post('/api/analyze', async (request, reply) => {
  const { ticker } = request.body;
  
  // 1. æ£€æŸ¥ç¼“å­˜
  const cached = await redisClient.get(`analysis:${ticker}`);
  if (cached) {
    return { success: true, data: JSON.parse(cached), from_cache: true };
  }
  
  // 2. å‘é€åˆ° RabbitMQ
  const connection = await amqp.connect('amqp://localhost');
  const channel = await connection.createChannel();
  await channel.assertQueue('analysis_tasks');
  
  const taskId = uuidv4();
  channel.sendToQueue('analysis_tasks', Buffer.from(JSON.stringify({
    task_id: taskId,
    ticker: ticker
  })));
  
  return { success: true, task_id: taskId, status: 'processing' };
});

fastify.listen({ port: 3000, host: '0.0.0.0' });
```

#### Python Worker (ä¿æŒä¸å˜)
```python
# backend/worker.py
import pika
import json
from analyzer import ButterflyAnalyzer
import redis

redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)

connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))
channel = connection.channel()
channel.queue_declare(queue='analysis_tasks')

def callback(ch, method, properties, body):
    task = json.loads(body)
    ticker = task['ticker']
    
    analyzer = ButterflyAnalyzer(ticker)
    result = analyzer.full_analysis()
    
    # å­˜å…¥ Redis + PostgreSQL
    redis_client.setex(f"analysis:{ticker}", 300, json.dumps(result))
    
    ch.basic_ack(delivery_tag=method.delivery_tag)

channel.basic_consume(queue='analysis_tasks', on_message_callback=callback)
channel.start_consuming()
```

#### ä¼˜åŠ¿
- **æ€§èƒ½**ï¼šNode.js å¯å¤„ç† **1000+ å¹¶å‘**
- **æˆæœ¬**ï¼šPython Worker å¯æŒ‰éœ€æ‰©å±•ï¼ˆKubernetes HPAï¼‰
- **ç¨³å®šæ€§**ï¼šè®¡ç®—å±‚å´©æºƒä¸å½±å“ API å±‚

#### æˆæœ¬ä¼°ç®—
- **é˜¿é‡Œäº‘ ACK (Kubernetes)**ï¼šçº¦ Â¥800-1200/æœˆ
  - Node.js Pod Ã— 3ï¼ˆ2æ ¸2Gï¼‰
  - Python Worker Pod Ã— 5ï¼ˆ4æ ¸8Gï¼‰
  - Redis Cluster Ã— 3
  - PostgreSQL RDS

---

### âš¡ æ–¹æ¡ˆ 3ï¼šå®Œå…¨é‡å†™ï¼ˆé•¿æœŸæ–¹æ¡ˆï¼‰

**æ ¸å¿ƒæ€è·¯**ï¼šç”¨ **Rust/Go** é‡å†™è®¡ç®—å¼•æ“

#### ä¸ºä»€ä¹ˆï¼Ÿ
- Python çš„ GILï¼ˆå…¨å±€è§£é‡Šå™¨é”ï¼‰é™åˆ¶äº†å¤šæ ¸åˆ©ç”¨ç‡
- Rust/Go çš„å¹¶å‘æ€§èƒ½æ˜¯ Python çš„ **10-100å€**
- ä½†å¼€å‘æˆæœ¬é«˜ï¼Œéœ€è¦é‡æ–°å®ç°æ‰€æœ‰é‡åŒ–æ¨¡å‹

#### æŠ€æœ¯æ ˆ
- **åç«¯**ï¼šRust (Actix-web) æˆ– Go (Gin)
- **è®¡ç®—åº“**ï¼š
  - Rust: `ndarray`, `polars`, `statrs`
  - Go: `gonum`, `gota`
- **éƒ¨ç½²**ï¼šå•äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œæ— éœ€ Python ç¯å¢ƒ

#### ç¤ºä¾‹ï¼ˆRustï¼‰
```rust
// src/analyzer.rs
use ndarray::Array1;
use statrs::distribution::Normal;

pub struct ButterflyAnalyzer {
    ticker: String,
    prices: Array1<f64>,
}

impl ButterflyAnalyzer {
    pub fn black_scholes(&self, s: f64, k: f64, t: f64, r: f64, sigma: f64) -> f64 {
        let d1 = ((s / k).ln() + (r + 0.5 * sigma.powi(2)) * t) / (sigma * t.sqrt());
        let d2 = d1 - sigma * t.sqrt();
        
        let normal = Normal::new(0.0, 1.0).unwrap();
        s * normal.cdf(d1) - k * (-r * t).exp() * normal.cdf(d2)
    }
}
```

#### ä¼˜åŠ¿
- **æ€§èƒ½**ï¼šå•æœºæ”¯æŒ **10,000+ å¹¶å‘**
- **æˆæœ¬**ï¼š1å° 4æ ¸8G æœåŠ¡å™¨å³å¯ï¼ˆÂ¥200/æœˆï¼‰

#### åŠ£åŠ¿
- **å¼€å‘å‘¨æœŸ**ï¼š3-6ä¸ªæœˆ
- **ç»´æŠ¤æˆæœ¬**ï¼šéœ€è¦ Rust/Go ä¸“å®¶

---

## ğŸ“ˆ æ¨èè·¯çº¿å›¾

### é˜¶æ®µ 1ï¼šå¿«é€Ÿä¸Šçº¿ï¼ˆ1-2å‘¨ï¼‰
âœ… é‡‡ç”¨ **æ–¹æ¡ˆ1**
- æ·»åŠ  Redis ç¼“å­˜
- å¼•å…¥ Celery å¼‚æ­¥ä»»åŠ¡
- éƒ¨ç½²åˆ° Docker Compose

### é˜¶æ®µ 2ï¼šä¼˜åŒ–æ€§èƒ½ï¼ˆ1-2æœˆï¼‰
âœ… é‡‡ç”¨ **æ–¹æ¡ˆ2**
- Node.js API å±‚
- Kubernetes éƒ¨ç½²
- ç›‘æ§å‘Šè­¦ï¼ˆPrometheus + Grafanaï¼‰

### é˜¶æ®µ 3ï¼šç»ˆæä¼˜åŒ–ï¼ˆ6æœˆ+ï¼‰
âœ… è¯„ä¼° **æ–¹æ¡ˆ3**
- æ ¹æ®ç”¨æˆ·å¢é•¿å†³å®šæ˜¯å¦é‡å†™
- å¦‚æœæ—¥æ´» < 10,000ï¼Œæ–¹æ¡ˆ2 å·²è¶³å¤Ÿ

---

## ğŸ”§ ç«‹å³å¯åšçš„ä¼˜åŒ–ï¼ˆæ— éœ€æ¶æ„æ”¹åŠ¨ï¼‰

### 1. å¯ç”¨ Gunicorn å¤šè¿›ç¨‹
```bash
# backend/Procfile
web: gunicorn -w 4 -k gevent --worker-connections 1000 app:app
```

### 2. æ·»åŠ ç®€å•ç¼“å­˜
```python
# backend/app.py
import time

CACHE = {}
CACHE_TTL = 300  # 5åˆ†é’Ÿ

@app.route('/api/analyze', methods=['POST'])
def analyze():
    ticker = request.json.get('ticker', 'AAPL').upper()
    
    # æ£€æŸ¥ç¼“å­˜
    if ticker in CACHE:
        cached_time, cached_result = CACHE[ticker]
        if time.time() - cached_time < CACHE_TTL:
            return jsonify({'success': True, 'data': cached_result, 'from_cache': True})
    
    # è®¡ç®—
    analyzer = ButterflyAnalyzer(ticker)
    result = analyzer.full_analysis()
    
    # å­˜å…¥ç¼“å­˜
    CACHE[ticker] = (time.time(), result)
    
    return jsonify({'success': True, 'data': result})
```

### 3. æ•°æ®åº“è¿æ¥æ± 
```python
# backend/database.py
from sqlalchemy import create_engine
from sqlalchemy.pool import QueuePool

engine = create_engine(
    'postgresql://user:pass@localhost/butterquant',
    poolclass=QueuePool,
    pool_size=10,
    max_overflow=20
)
```

---

## ğŸ’° æˆæœ¬å¯¹æ¯”

| æ–¹æ¡ˆ | æœˆæˆæœ¬ | æ”¯æŒå¹¶å‘ | å¼€å‘å‘¨æœŸ | ç»´æŠ¤éš¾åº¦ |
|------|--------|----------|----------|----------|
| å½“å‰æ¶æ„ | Â¥0 | 10 | - | ä½ |
| æ–¹æ¡ˆ1 (Celery) | Â¥300-500 | 100-200 | 1-2å‘¨ | ä¸­ |
| æ–¹æ¡ˆ2 (Node.js) | Â¥800-1200 | 1000+ | 1-2æœˆ | ä¸­é«˜ |
| æ–¹æ¡ˆ3 (Rust) | Â¥200-400 | 10000+ | 6æœˆ+ | é«˜ |

---

## ğŸ¯ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **ç¡®å®šç›®æ ‡ç”¨æˆ·è§„æ¨¡**ï¼šé¢„è®¡æ—¥æ´»å¤šå°‘ï¼Ÿ
2. **é€‰æ‹©æ–¹æ¡ˆ**ï¼šå»ºè®®å…ˆåšæ–¹æ¡ˆ1ï¼Œå¿«é€ŸéªŒè¯å¸‚åœº
3. **å¯ä»¥å¸®æ‚¨**ï¼š
   - ç”Ÿæˆå®Œæ•´çš„ Docker Compose é…ç½®
   - æ”¹é€ ç°æœ‰ä»£ç æ”¯æŒ Celery
   - ç¼–å†™éƒ¨ç½²è„šæœ¬

---

## ğŸ“ æ€»ç»“

- **ç«‹å³ä¼˜åŒ–**ï¼šä»Šå¤©å°±èƒ½å®Œæˆï¼Œæ€§èƒ½æå‡ 2-3å€
- **æ–¹æ¡ˆ1**ï¼š1-2å‘¨ä¸Šçº¿ï¼Œæ”¯æŒ 100-200 å¹¶å‘ï¼Œæˆæœ¬ Â¥300-500/æœˆ
- **æ–¹æ¡ˆ2**ï¼š1-2æœˆå®Œæˆï¼Œæ”¯æŒ 1000+ å¹¶å‘ï¼Œæˆæœ¬ Â¥800-1200/æœˆ
- **æ–¹æ¡ˆ3**ï¼š6æœˆ+å®Œæˆï¼Œæ”¯æŒ 10000+ å¹¶å‘ï¼Œæˆæœ¬ Â¥200-400/æœˆ

å»ºè®®è·¯å¾„ï¼š**ç«‹å³ä¼˜åŒ– â†’ æ–¹æ¡ˆ1 â†’ æ ¹æ®ç”¨æˆ·å¢é•¿å†³å®šæ˜¯å¦å‡çº§åˆ°æ–¹æ¡ˆ2**
