"""
ButterQuant ML Inference Engine V2.0
4åˆ†ç±»æ¨ç†å¼•æ“ + æœŸæœ›ROIè®¡ç®—

æ ¸å¿ƒåŠŸèƒ½:
1. 4åˆ†ç±»æ¦‚ç‡åˆ†å¸ƒé¢„æµ‹
2. æœŸæœ›ROIè®¡ç®—: E[ROI] = Î£(p_i Ã— roi_i)
3. ONNX Runtimeä¼˜åŒ– (CPU/CUDA)
4. é«˜æ€§èƒ½æ¨ç† (<2ms/æ ·æœ¬)
"""

import numpy as np
import joblib
from pathlib import Path
from typing import Dict, Optional
import logging

try:
    import onnxruntime as ort
    ONNX_AVAILABLE = True
except ImportError:
    ONNX_AVAILABLE = False
    logging.warning("âš ï¸ onnxruntimeæœªå®‰è£…")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MLInferenceEngine:
    """MLæ¨ç†å¼•æ“ V2.0"""
    
    # ROIåˆ†çº§å®šä¹‰
    ROI_VALUES = [0.0, 0.05, 0.20, 0.40]  # å¯¹åº”0/1/2/3ç±»
    
    def __init__(self, model_dir: str = "ml", use_cuda: bool = True):
        self.model_dir = Path(model_dir)
        self.use_cuda = use_cuda and ONNX_AVAILABLE
        
        self._session = None
        self._scaler = None
        
        from ml.features import FeatureExtractor
        self.feature_names = FeatureExtractor.FEATURE_NAMES
        
        self.load_model()
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹å’Œé¢„å¤„ç†å™¨"""
        logger.info("ğŸ”„ åŠ è½½MLæ¨ç†å¼•æ“...")
        
        # åŠ è½½Scaler
        scaler_path = self.model_dir / "scaler_v2.pkl"
        if not scaler_path.exists():
            scaler_path = self.model_dir / "scaler.pkl"
        
        if scaler_path.exists():
            self._scaler = joblib.load(scaler_path)
            logger.info(f"âœ… ScaleråŠ è½½æˆåŠŸ: {scaler_path}")
        else:
            raise FileNotFoundError(f"Scaler not found: {scaler_path}")
        
        # åŠ è½½ONNXæ¨¡å‹
        onnx_path = self.model_dir / "success_model_v2.onnx"
        if ONNX_AVAILABLE and onnx_path.exists():
            providers = []
            if self.use_cuda:
                providers.append('CUDAExecutionProvider')
            providers.append('CPUExecutionProvider')
            
            self._session = ort.InferenceSession(str(onnx_path), providers=providers)
            
            actual_provider = self._session.get_providers()[0]
            logger.info(f"âœ… ONNXæ¨¡å‹åŠ è½½æˆåŠŸ: {onnx_path}")
            logger.info(f"   Provider: {actual_provider}")
            
            if 'CUDA' in actual_provider:
                logger.info(f"   ğŸš€ GPUåŠ é€Ÿå·²å¯ç”¨")
        else:
            raise FileNotFoundError(f"ONNX model not found: {onnx_path}")
    
    def predict_roi_distribution(self, features_dict: Dict) -> Optional[Dict]:
        """
        é¢„æµ‹ROIæ¦‚ç‡åˆ†å¸ƒ
        
        è¿”å›:
            {
                'prob_loss': float,
                'prob_minor': float,
                'prob_good': float,
                'prob_excellent': float,
                'expected_roi': float,
                'confidence': float,
                'predicted_class': int,
                'class_name': str
            }
        """
        if self._session is None or self._scaler is None:
            logger.error("âŒ æ¨¡å‹æœªåŠ è½½")
            return None
        
        try:
            # æ„å»ºç‰¹å¾å‘é‡
            X = np.array([
                features_dict.get(name, 0.0) 
                for name in self.feature_names
            ], dtype=np.float32).reshape(1, -1)
            
            # å¤„ç†å¼‚å¸¸å€¼
            if not np.isfinite(X).all():
                logger.warning("âš ï¸ ç‰¹å¾åŒ…å«NaN/Inf")
                X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
            
            # æ ‡å‡†åŒ–
            X_scaled = self._scaler.transform(X).astype(np.float32)
            
            # ONNXæ¨ç†
            input_name = self._session.get_inputs()[0].name
            logits = self._session.run(None, {input_name: X_scaled})[0][0]
            
            # Softmax
            probs = self._softmax(logits)
            
            # è®¡ç®—æœŸæœ›ROI
            expected_roi = np.dot(probs, self.ROI_VALUES)
            
            # é¢„æµ‹ç±»åˆ«
            predicted_class = int(np.argmax(probs))
            confidence = float(np.max(probs))
            
            return {
                'prob_loss': float(probs[0]),
                'prob_minor': float(probs[1]),
                'prob_good': float(probs[2]),
                'prob_excellent': float(probs[3]),
                'expected_roi': float(expected_roi),
                'confidence': confidence,
                'predicted_class': predicted_class,
                'class_name': self._get_class_name(predicted_class)
            }
        
        except Exception as e:
            logger.error(f"âŒ æ¨ç†å¤±è´¥: {e}")
            return None
    
    @staticmethod
    def _softmax(logits: np.ndarray) -> np.ndarray:
        """Softmaxå‡½æ•°"""
        exp_logits = np.exp(logits - np.max(logits))
        return exp_logits / exp_logits.sum()
    
    @staticmethod
    def _get_class_name(class_idx: int) -> str:
        """è·å–ç±»åˆ«åç§°"""
        names = ['loss', 'minor', 'good', 'excellent']
        return names[class_idx] if 0 <= class_idx < 4 else 'unknown'


# ==================== å…¨å±€å•ä¾‹ ====================

_global_engine = None

def get_inference_engine(model_dir: str = "ml", use_cuda: bool = True) -> MLInferenceEngine:
    """è·å–å…¨å±€æ¨ç†å¼•æ“ (å•ä¾‹æ¨¡å¼)"""
    global _global_engine
    
    if _global_engine is None:
        _global_engine = MLInferenceEngine(model_dir, use_cuda)
    
    return _global_engine


def predict_roi(features_dict: Dict) -> Optional[Dict]:
    """å¿«æ·å‡½æ•°: ç›´æ¥é¢„æµ‹ROIåˆ†å¸ƒ"""
    engine = get_inference_engine()
    return engine.predict_roi_distribution(features_dict)


def should_execute_trade(features_dict: Dict, min_expected_roi: float = 0.15) -> bool:
    """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ‰§è¡Œäº¤æ˜“"""
    result = predict_roi(features_dict)
    
    if result is None:
        return False
    
    return result['expected_roi'] >= min_expected_roi


# ==================== æµ‹è¯•ä»£ç  ====================

if __name__ == "__main__":
    print("=" * 60)
    print("MLæ¨ç†å¼•æ“ V2.0 - æµ‹è¯•")
    print("=" * 60)
    
    # åˆå§‹åŒ–å¼•æ“
    engine = MLInferenceEngine(use_cuda=True)
    
    # æµ‹è¯•æ¨ç†
    print("\nğŸ§ª æµ‹è¯•æ¨ç†...")
    
    from ml.features import FeatureExtractor
    mock_features = {name: np.random.randn() for name in FeatureExtractor.FEATURE_NAMES}
    
    result = engine.predict_roi_distribution(mock_features)
    
    if result:
        print("\nâœ… æ¨ç†æˆåŠŸ!")
        print(f"\né¢„æµ‹ç»“æœ:")
        print(f"  P(äºæŸ):   {result['prob_loss']:.2%}")
        print(f"  P(å¾®åˆ©):   {result['prob_minor']:.2%}")
        print(f"  P(è‰¯å¥½):   {result['prob_good']:.2%}")
        print(f"  P(ä¼˜ç§€):   {result['prob_excellent']:.2%}")
        print(f"\n  æœŸæœ›ROI:   {result['expected_roi']:.2%}")
        print(f"  é¢„æµ‹ç±»åˆ«:  {result['class_name']} (ç½®ä¿¡åº¦: {result['confidence']:.2%})")
        
        if result['expected_roi'] >= 0.15:
            print(f"\n  âœ… å»ºè®®æ‰§è¡Œ (æœŸæœ›ROI > 15%)")
        else:
            print(f"\n  âŒ ä¸å»ºè®®æ‰§è¡Œ (æœŸæœ›ROI < 15%)")
    else:
        print("\nâŒ æ¨ç†å¤±è´¥")