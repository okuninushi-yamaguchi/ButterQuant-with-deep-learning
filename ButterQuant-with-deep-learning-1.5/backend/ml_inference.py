# -*- coding: utf-8 -*-
"""
ButterQuant ML Inference Engine V2.0 / ButterQuant ML æ¨ç†å¼•æ“ V2.0
4åˆ†ç±»æ¨ç†å¼•æ“ + æœŸæœ›ROIè®¡ç®— / 4-class inference + Expected ROI calculation

æ ¸å¿ƒåŠŸèƒ½ / Core Features:
1. ä»äºŒåˆ†ç±»æ¦‚ç‡ â†’ 4åˆ†ç±»æ¦‚ç‡åˆ†å¸ƒ / Binary â†’ 4-class probability distribution
2. è®¡ç®—æœŸæœ›ROI: E[ROI] = Î£(p_i Ã— roi_i) / Calculate expected ROI
3. æ”¯æŒONNX Runtime (CPU/CUDA) / ONNX Runtime support
4. é«˜æ€§èƒ½æ¨ç† (<2ms/æ ·æœ¬) / High-performance inference

ä½¿ç”¨ç¤ºä¾‹ / Example:
    engine = MLInferenceEngine()
    result = engine.predict_roi_distribution(features_dict)
    
    if result['expected_roi'] > 0.15:
        execute_trade()  # æœŸæœ›ROI > 15%, æ‰§è¡Œäº¤æ˜“ / Execute trade
"""

import numpy as np
import joblib
import logging
import os
from pathlib import Path
from typing import Dict, Optional, List

# ä½¿ç”¨ onnxruntime è¿›è¡Œæ¨ç† / Use onnxruntime for inference
try:
    import onnxruntime as ort
    HAS_ONNX = True
except ImportError:
    HAS_ONNX = False

# è®¾ç½®æ—¥å¿— / Setup Logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ModelInference:
    """
    MLæ¨ç†å¼•æ“ (å•ä¾‹æ¨¡å¼) / ML Inference Engine (Singleton)
    
    æ”¯æŒä¸¤ç§æ¨¡å‹ / Supports two models:
    - V2 (4åˆ†ç±»): è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒå’ŒæœŸæœ›ROI / Outputs probability distribution and expected ROI
    - V1 (äºŒåˆ†ç±»): å‘åå…¼å®¹,è¾“å‡ºæˆåŠŸæ¦‚ç‡ / Backward compatible, outputs success probability
    """
    
    # ROIåˆ†çº§å®šä¹‰ (å¯¹åº”0/1/2/3ç±»åˆ«) / ROI levels (for classes 0/1/2/3)
    ROI_VALUES = [-0.10, 0.05, 0.20, 0.40]  # äºæŸ, å¾®åˆ©5%, è‰¯å¥½20%, ä¼˜ç§€40%
    
    _instance = None
    _session_v2 = None
    _scaler_v2 = None
    _session_v1 = None
    _scaler_v1 = None
    _model_version = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(ModelInference, cls).__new__(cls)
            cls._instance._load_resources()
        return cls._instance

    def _load_resources(self):
        """åŠ è½½æ¨¡å‹å’Œç¼©æ”¾å™¨èµ„æº / Load model and scaler resources"""
        base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        
        # å°è¯•åŠ è½½V2æ¨¡å‹ (4åˆ†ç±») / Try loading V2 model (4-class)
        onnx_path_v2 = os.path.join(base_dir, 'ml', 'models', 'success_model_v2.onnx')
        scaler_path_v2 = os.path.join(base_dir, 'ml', 'models', 'scaler_v2.joblib')
        
        # å¤‡ç”¨scalerè·¯å¾„ / Alternative scaler path
        scaler_path_v2_alt = os.path.join(base_dir, 'ml', 'models', 'scaler_v2.pkl')
        
        # å°è¯•åŠ è½½V1æ¨¡å‹ (äºŒåˆ†ç±») / Try loading V1 model (binary)
        onnx_path_v1 = os.path.join(base_dir, 'ml', 'models', 'success_model.onnx')
        scaler_path_v1 = os.path.join(base_dir, 'ml', 'models', 'scaler.joblib')
        
        try:
            # ä¼˜å…ˆåŠ è½½V2 / Prefer V2
            if os.path.exists(onnx_path_v2):
                # å°è¯•å¤šä¸ªscalerè·¯å¾„ / Try multiple scaler paths
                if os.path.exists(scaler_path_v2):
                    self._scaler_v2 = joblib.load(scaler_path_v2)
                elif os.path.exists(scaler_path_v2_alt):
                    self._scaler_v2 = joblib.load(scaler_path_v2_alt)
                
                if self._scaler_v2 is not None and HAS_ONNX:
                    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if ort.get_device() == 'GPU' else ['CPUExecutionProvider']
                    self._session_v2 = ort.InferenceSession(onnx_path_v2, providers=providers)
                    self._model_version = 'V2'
                    logger.info(f"âœ… V2æ¨¡å‹åŠ è½½æˆåŠŸ / V2 Model loaded with {self._session_v2.get_providers()[0]}")
            
            # åŒæ—¶åŠ è½½V1ç”¨äºå‘åå…¼å®¹ / Also load V1 for backward compatibility
            if os.path.exists(onnx_path_v1) and os.path.exists(scaler_path_v1):
                self._scaler_v1 = joblib.load(scaler_path_v1)
                
                if HAS_ONNX:
                    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if ort.get_device() == 'GPU' else ['CPUExecutionProvider']
                    self._session_v1 = ort.InferenceSession(onnx_path_v1, providers=providers)
                    
                    if self._model_version is None:
                        self._model_version = 'V1'
                        logger.info(f"âœ… V1æ¨¡å‹åŠ è½½æˆåŠŸ / V1 Model loaded (fallback)")
            
            if self._model_version is None:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•MLæ¨¡å‹,è¿›å…¥å›é€€æ¨¡å¼ / No ML model found, running in fallback mode")
                    
        except Exception as e:
            logger.error(f"åŠ è½½ ML èµ„æºå¤±è´¥: {e} / Failed to load ML resources")
            self._session_v2 = None
            self._session_v1 = None

    def predict_roi_distribution(self, features_dict: Dict) -> Optional[Dict]:
        """
        é¢„æµ‹4ä¸ªç­‰çº§çš„æ¦‚ç‡åˆ†å¸ƒ (V2æ¨¡å‹) / Predict 4-class probability distribution (V2 model)
        
        è¾“å…¥ / Input: ç‰¹å¾å­—å…¸ (23ç»´) / Feature dictionary (23-dim)
        è¾“å‡º / Output: {
            'prob_loss': float,      # P(äºæŸ) / P(Loss)
            'prob_minor': float,     # P(å¾®åˆ©) / P(Minor profit)
            'prob_good': float,      # P(è‰¯å¥½) / P(Good)
            'prob_excellent': float, # P(ä¼˜ç§€) / P(Excellent)
            'expected_roi': float,   # æœŸæœ›ROI / Expected ROI
            'confidence': float,     # é¢„æµ‹ç½®ä¿¡åº¦ / Prediction confidence
            'predicted_class': int,  # é¢„æµ‹ç±»åˆ« / Predicted class
            'class_name': str        # ç±»åˆ«åç§° / Class name
        }
        """
        if self._session_v2 is None or self._scaler_v2 is None:
            # V2æ¨¡å‹ä¸å¯ç”¨,å°è¯•å›é€€åˆ°V1 / V2 not available, fallback to V1
            prob = self.predict_success_probability(features_dict)
            if prob is not None:
                # å°†äºŒåˆ†ç±»ç»“æœè½¬æ¢ä¸º4åˆ†ç±»è¿‘ä¼¼ / Convert binary to 4-class approximation
                return {
                    'prob_loss': 1.0 - prob,
                    'prob_minor': prob * 0.4,
                    'prob_good': prob * 0.4,
                    'prob_excellent': prob * 0.2,
                    'expected_roi': prob * 0.15,  # è¿‘ä¼¼ / Approximate
                    'confidence': max(prob, 1.0 - prob),
                    'predicted_class': 2 if prob > 0.5 else 0,
                    'class_name': 'good' if prob > 0.5 else 'loss'
                }
            return None

        try:
            # å¯¼å…¥ç‰¹å¾ååˆ—è¡¨ / Import feature names
            try:
                from ml.features import FeatureExtractor
                feature_cols = FeatureExtractor.FEATURE_NAMES
            except ImportError:
                # å›é€€åˆ°ç¡¬ç¼–ç åˆ—è¡¨ / Fallback to hardcoded list
                feature_cols = [
                    'trend_slope', 'dominant_period', 'period_strength', 'forecast_price',
                    'predicted_vol', 'current_iv', 'vol_mispricing', 'iv_percentile',
                    'delta', 'gamma', 'vega', 'theta', 'max_profit', 'max_loss',
                    'profit_ratio', 'prob_profit',
                    'skew_estimate', 'momentum_7d', 'vol_concentration', 
                    'dte_factor', 'price_stability', 'gamma_theta_ratio'
                ]
            
            # æ„å»ºç‰¹å¾å‘é‡ / Build feature vector
            feature_vector = []
            for col in feature_cols:
                val = features_dict.get(col, 0.0)
                if val is None:
                    val = 0.0
                # å¤„ç†æ— ç©·å€¼ / Handle infinite values
                if not np.isfinite(val):
                    val = 0.0
                feature_vector.append(float(val))
            
            # å˜å½¢å’Œç¼©æ”¾ / Reshape & Scale
            X = np.array(feature_vector).reshape(1, -1)
            X_scaled = self._scaler_v2.transform(X).astype(np.float32)
            
            # ONNXæ¨ç† / ONNX Inference
            inputs = {self._session_v2.get_inputs()[0].name: X_scaled}
            outputs = self._session_v2.run(None, inputs)
            
            # è¾“å‡ºæ˜¯logits,éœ€è¦softmax / Output is logits, need softmax
            logits = outputs[0][0]  # shape: (4,)
            exp_logits = np.exp(logits - np.max(logits))  # æ•°å€¼ç¨³å®š / Numerical stability
            probs = exp_logits / np.sum(exp_logits)
            
            # è®¡ç®—æœŸæœ›ROI / Calculate expected ROI
            expected_roi = float(np.dot(probs, self.ROI_VALUES))
            
            # é¢„æµ‹ç±»åˆ«å’Œç½®ä¿¡åº¦ / Predicted class and confidence
            predicted_class = int(np.argmax(probs))
            confidence = float(np.max(probs))
            
            return {
                'prob_loss': float(probs[0]),
                'prob_minor': float(probs[1]),
                'prob_good': float(probs[2]),
                'prob_excellent': float(probs[3]),
                'expected_roi': expected_roi,
                'confidence': confidence,
                'predicted_class': predicted_class,
                'class_name': self._get_class_name(predicted_class)
            }
            
        except Exception as e:
            logger.error(f"V2é¢„æµ‹å¤±è´¥: {e} / V2 Prediction failed")
            return None

    def predict_success_probability(self, features_dict: Dict) -> Optional[float]:
        """
        [å‘åå…¼å®¹] é¢„æµ‹è¶å¼ç­–ç•¥çš„æˆåŠŸæ¦‚ç‡ (V1æ¨¡å‹) / [Backward compatible] Predict success probability
        
        è¾“å…¥ / Input: ç‰¹å¾å­—å…¸ / Feature dictionary
        è¾“å‡º / Output: æµ®ç‚¹æ•° (0.0 to 1.0) / Float (0.0 to 1.0)
        """
        # å¦‚æœåªæœ‰V2å¯ç”¨,ä»V2ç»“æœè®¡ç®— / If only V2 available, compute from V2 result
        if self._session_v1 is None and self._session_v2 is not None:
            result = self.predict_roi_distribution(features_dict)
            if result:
                # æˆåŠŸæ¦‚ç‡ = 1 - P(äºæŸ) / Success prob = 1 - P(Loss)
                return 1.0 - result['prob_loss']
            return None
        
        if self._session_v1 is None or self._scaler_v1 is None:
            return None

        try:
            # V1ç‰¹å¾é¡ºåº (17ç»´,åŒ…å«total_score) / V1 feature order (17-dim, includes total_score)
            feature_cols = [
                'trend_slope', 'dominant_period', 'period_strength', 'forecast_price',
                'predicted_vol', 'current_iv', 'vol_mispricing', 'iv_percentile',
                'delta', 'gamma', 'vega', 'theta', 'max_profit', 'max_loss',
                'profit_ratio', 'prob_profit', 'total_score'
            ]
            
            # æŒ‰æ­£ç¡®é¡ºåºå°†å­—å…¸è½¬æ¢ä¸ºæ•°ç»„ / Convert dictionary to array in correct order
            feature_vector = []
            for col in feature_cols:
                val = features_dict.get(col, 0.0) 
                if val is None:
                    val = 0.0
                feature_vector.append(float(val))

            # å˜å½¢å’Œç¼©æ”¾ / Reshape & Scale
            X = np.array(feature_vector).reshape(1, -1)
            X_scaled = self._scaler_v1.transform(X).astype(np.float32)
            
            # æ‰§è¡Œæ¨ç† / Run Inference
            inputs = {self._session_v1.get_inputs()[0].name: X_scaled}
            outputs = self._session_v1.run(None, inputs)
            
            prob = float(outputs[0][0][0])
            return prob
            
        except Exception as e:
            logger.error(f"V1é¢„æµ‹å¤±è´¥: {e} / V1 Prediction failed")
            return None
    
    def batch_predict(self, features_list: List[Dict]) -> List[Optional[Dict]]:
        """
        æ‰¹é‡æ¨ç† / Batch inference
        
        å‚æ•° / Parameters:
            features_list: List[Dict] - ç‰¹å¾å­—å…¸åˆ—è¡¨ / List of feature dicts
        
        è¿”å› / Returns:
            List[Dict] - é¢„æµ‹ç»“æœåˆ—è¡¨ / List of prediction results
        """
        return [self.predict_roi_distribution(f) for f in features_list]
    
    def benchmark(self, n_samples: int = 1000):
        """
        æ€§èƒ½åŸºå‡†æµ‹è¯• / Performance benchmark
        
        å‚æ•° / Parameters:
            n_samples: æµ‹è¯•æ ·æœ¬æ•° / Number of test samples
        """
        import time
        
        logger.info(f"\nâ±ï¸ æ€§èƒ½åŸºå‡†æµ‹è¯• ({n_samples} æ ·æœ¬) / Performance benchmark")
        
        # ç”Ÿæˆéšæœºç‰¹å¾ / Generate random features
        try:
            from ml.features import FeatureExtractor
            feature_names = FeatureExtractor.FEATURE_NAMES
        except ImportError:
            feature_names = ['trend_slope', 'dominant_period', 'period_strength', 'forecast_price',
                           'predicted_vol', 'current_iv', 'vol_mispricing', 'iv_percentile',
                           'delta', 'gamma', 'vega', 'theta', 'max_profit', 'max_loss',
                           'profit_ratio', 'prob_profit', 'skew_estimate', 'momentum_7d',
                           'vol_concentration', 'dte_factor', 'price_stability', 'gamma_theta_ratio']
        
        dummy_features = {name: np.random.randn() for name in feature_names}
        
        # é¢„çƒ­ / Warmup
        for _ in range(10):
            self.predict_roi_distribution(dummy_features)
        
        # æµ‹è¯• / Test
        start = time.time()
        for _ in range(n_samples):
            self.predict_roi_distribution(dummy_features)
        elapsed = time.time() - start
        
        avg_time = elapsed / n_samples * 1000  # ms
        throughput = n_samples / elapsed
        
        logger.info(f"   æ€»è€—æ—¶ / Total: {elapsed:.2f}s")
        logger.info(f"   å¹³å‡å»¶è¿Ÿ / Avg latency: {avg_time:.2f}ms/æ ·æœ¬")
        logger.info(f"   ååé‡ / Throughput: {throughput:.0f} æ ·æœ¬/ç§’")
        
        if avg_time < 2.0:
            logger.info(f"   âœ… æ€§èƒ½è¾¾æ ‡ (ç›®æ ‡<2ms) / Performance OK")
        else:
            logger.warning(f"   âš ï¸ æ€§èƒ½æœªè¾¾æ ‡ (ç›®æ ‡<2ms) / Performance below target")
    
    def get_model_version(self) -> Optional[str]:
        """è·å–å½“å‰åŠ è½½çš„æ¨¡å‹ç‰ˆæœ¬ / Get loaded model version"""
        return self._model_version
    
    @staticmethod
    def _get_class_name(class_idx: int) -> str:
        """è·å–ç±»åˆ«åç§° / Get class name"""
        names = ['loss', 'minor', 'good', 'excellent']
        return names[class_idx] if 0 <= class_idx < 4 else 'unknown'


class ModelInferenceWithCache(ModelInference):
    """
    å¸¦ç¼“å­˜çš„æ¨ç†å¼•æ“ / Inference engine with caching
    
    å¯¹äºç›¸åŒçš„ç‰¹å¾, ç›´æ¥è¿”å›ç¼“å­˜ç»“æœ / Returns cached results for same features
    """
    
    def __init__(self, cache_size: int = 1000):
        super().__new__(ModelInference)  # ä½¿ç”¨çˆ¶ç±»å•ä¾‹ / Use parent singleton
        self._cache = {}
        self._cache_size = cache_size
    
    def predict_roi_distribution(self, features_dict: Dict) -> Optional[Dict]:
        """å¸¦ç¼“å­˜çš„æ¨ç† / Cached inference"""
        # ç”Ÿæˆç¼“å­˜key / Generate cache key
        cache_key = self._make_cache_key(features_dict)
        
        # æ£€æŸ¥ç¼“å­˜ / Check cache
        if cache_key in self._cache:
            return self._cache[cache_key]
        
        # æ¨ç† / Inference
        result = super().predict_roi_distribution(features_dict)
        
        # å­˜å…¥ç¼“å­˜ / Store in cache
        if result is not None:
            if len(self._cache) >= self._cache_size:
                # LRU: åˆ é™¤æœ€æ—©çš„é¡¹ / Delete oldest item
                self._cache.pop(next(iter(self._cache)))
            self._cache[cache_key] = result
        
        return result
    
    def _make_cache_key(self, features_dict: Dict) -> str:
        """ç”Ÿæˆç¼“å­˜key / Generate cache key"""
        try:
            from ml.features import FeatureExtractor
            feature_names = FeatureExtractor.FEATURE_NAMES
        except ImportError:
            feature_names = list(features_dict.keys())
        values = tuple(features_dict.get(name, 0.0) for name in feature_names)
        return str(hash(values))
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜ / Clear cache"""
        self._cache.clear()
        logger.info("ğŸ—‘ï¸ ç¼“å­˜å·²æ¸…ç©º / Cache cleared")


# ==================== ä¾¿æ·å‡½æ•° / Convenience Functions ====================

_global_engine = None


def get_inference_engine(use_cache: bool = False) -> ModelInference:
    """
    è·å–å…¨å±€æ¨ç†å¼•æ“ (å•ä¾‹æ¨¡å¼) / Get global inference engine (singleton)
    
    ä½¿ç”¨ç¤ºä¾‹ / Example:
        engine = get_inference_engine()
        result = engine.predict_roi_distribution(features)
    """
    global _global_engine
    
    if _global_engine is None:
        if use_cache:
            _global_engine = ModelInferenceWithCache()
        else:
            _global_engine = ModelInference()
    
    return _global_engine


def predict_roi(features_dict: Dict) -> Optional[Dict]:
    """
    å¿«æ·å‡½æ•°: ç›´æ¥é¢„æµ‹ROIåˆ†å¸ƒ / Shortcut: Predict ROI distribution directly
    
    å‚æ•° / Parameters:
        features_dict: ç‰¹å¾å­—å…¸ / Feature dictionary
    
    è¿”å› / Returns:
        é¢„æµ‹ç»“æœå­—å…¸ / Prediction result dictionary
    """
    engine = get_inference_engine()
    return engine.predict_roi_distribution(features_dict)


def should_execute_trade(features_dict: Dict, min_expected_roi: float = 0.15) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦åº”è¯¥æ‰§è¡Œäº¤æ˜“ / Determine if trade should be executed
    
    å‚æ•° / Parameters:
        features_dict: ç‰¹å¾å­—å…¸ / Feature dictionary
        min_expected_roi: æœ€å°æœŸæœ›ROIé˜ˆå€¼ (é»˜è®¤15%) / Min expected ROI threshold (default 15%)
    
    è¿”å› / Returns:
        bool: Trueè¡¨ç¤ºåº”è¯¥æ‰§è¡Œ / True means should execute
    """
    result = predict_roi(features_dict)
    
    if result is None:
        return False
    
    return result['expected_roi'] >= min_expected_roi


# å•ä¾‹å®ä¾‹ / Singleton instance
ml_engine = ModelInference()
